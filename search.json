[{"title":"GPGPU-Sim 3.x 中文手册","url":"/2022/12/21/GPGPU-Sim-3-x-%E4%B8%AD%E6%96%87%E6%89%8B%E5%86%8C/","content":"Website: http://gpgpu-sim.org/manual/index.php/Main_Page\n\nRevision 1.2 (GPGPU-Sim 3.1.1)\n\nEditors: TorM. Aamodt, WilsonW. L. Fung, TaylerH. Hetherington\n\nThis page was last modified on 13 June 2017, at 14:53.\nThis page has been accessed 259,808 times.\n\n第一部分：概述本手册提供了GPGPU-Sim 3.x的文档，这是一个循环级GPU性能模拟器，专注于“GPU计算”（GPU上的通用计算）。GPGPU-Sim 3.x是GPGPU-Sim的最新版本。它包括对GPGPU-Sim 2.x的许多增强。如果您正在尝试安装GPGPU-Sim，请参阅您正在使用的GPGPU模拟发行版中的README文件。\n手册包含三个主要部分：\n\n微架构模型部分：描述GPGPU-Sim 3.x建模的微架构。\n\n使用部分：提供有关如何使用GPGPU-Sim的文档。提供以下信息：\n\n不同的模拟模式\n\n配置选项（如何更改模拟微架构的高级参数）\n\n模拟输出（例如，微体系结构统计）\n\n可视化微体系结构行为（对性能调试有用）\n\n当性能模拟因时序模型错误而崩溃或死锁时，调试GPGPU-Sim的策略。\n\n\n\n软件设计部分解释了GPGPU-Sim 3.x的内部软件设计。该部分的目标是为用户提供一个起点，以扩展GPGPU-Sim进行自己的研究。\n\n\nISCA 2012教程幻灯片\n\n第二部分：微架构模型部分当配置为使用本机硬件指令集（PTXPlus，使用-gpgpu_ptx_convert_to_PTXPlus选项）时的模拟性能会更准确。\n一、概述（一）Top架构GPU由单指令多线程（SIMT）Cores组成，这些Cores通过片上连接网络连接到与图形GDDR DRAM接口的内存分区。SIMD处理器大致相当于NVIDIA所说的流式处理多处理器（SM）。GPU体系结构：\n\n\n\n（二）时钟域GPGPU-Sim支持四个独立的时钟域：（1）SIMT Core集群时钟域（2）互连网络时钟域（3）L2高速缓存时钟域，适用于内存分区单元中除DRAM之外的所有逻辑（4）DRAM时钟域。\n时钟频率可以具有任意值（它们不需要是彼此的倍数）。换句话说，GPGPU-Sim假设时钟域之间存在同步器。在GPGPU-Sim 3.x仿真模型中，相邻时钟域中的单元通过时钟交叉缓冲区进行通信，这些缓冲区以源域的时钟速率填充，并以目标域的时钟速率耗尽。\n\n二、SIMT Core集群多个SIMT Core构成一个SIMT Core集群的分组，也就是说，GPU内部有多个SIMT Core集群，每个集群包含几个SIMT Core。SIMT Core集群中的SIMT Core与互连网络共享一个公共端口，如下图所示：\n\n每个SIMT Core集群都有一个响应FIFO，用于保存从互连网络发出的数据包。数据包被定向到SIMT Core的指令缓存（如果它是为指令获取未命中提供服务的内存响应）或其内存流水线（memory pipeline，LDST 单元）。数据包以先进先出方式拿出。如果SIMT Core无法接受FIFO头部的数据包，则响应FIFO将停止。为了在LDST单元上生成内存请求，每个SIMT Core都有自己的注入端口接入互连网络。但是，注入端口缓冲区由SIMT Core集群所有SIMT Core共享。\n\n三、SIMT CoresSIMT Core对高度多线程流水线SIMD处理器进行建模，大致相当于NVIDIA所说的流式多处理器 （SM）或AMD所说的计算单元（CU）。流处理器 （SP）或CUDA Core将对应于SIMT Core中ALU流水线内的pipeline。下图说明了GPGPU-Sim 3.x模拟的SIMT Core微体系结构：\n\n\n（一）前端如下所述，前端的主要阶段包括指令缓存访问和指令缓冲逻辑、记分牌和调度逻辑、SIMT堆栈。\n\n1. Fetch和Decode上图的指令缓冲区（I-Buffer）用于在从指令高速缓存（I-Cache）中获取指令后对其进行缓冲。它是静态分区的，因此在SIMT Core上运行的所有warp都有专用的存储来放置指令。在当前模型中，每个warp都有两个I-Buffer entries。每个I-Buffer entry都有一个有效的位（valid）、就绪位（ready）和一个用于此warp的解码后的指令（Inst. W1或Inst. W2）。Entries的有效位指示I-Buffer中的此entry中存在未发出的解码后的指令（Inst. W1或Inst. W2）。而就绪位表示此warp的解码指令已准备好发出给执行流水线。从概念上讲，就绪位是使用记分牌逻辑和硬件资源是否可用状态在调度（schedule）和发射（issue）阶段设置的（在模拟器软件中，并非真实地设置就绪位，而是执行是否准备就绪的情况检查）。I-Buffer最初是空的，在初始状态所有有效位和就绪位均无效。\n如果Warp的I-Buffer中没有任何有效指令，则它有资格获取新指令。符合条件的warps被安排以 轮循机制 顺序访问指令缓存（I-Cache）。一旦某个warp被选定为当前轮到获取指令，则它会把读取请求发送到指令缓存，读取请求其中包含当前调度到的warp中下一条指令的地址。默认情况下，将获取两条连续的指令。一旦调度器为某个warp调度到了fetch指令的机会，I-Buffer中的有效位就会被激活，直到该warp的所有获取指令都发出到执行流水线。\n指令高速缓存（I-Cache）是只读、非阻塞的集合关联高速缓存，可以使用未命中或填充分配策略对FIFO和LRU替换策略建模。对指令高速缓存的请求会导致命中（hit）、未命中（miss）或预订失败（reservation fail）。如果未命中状态保持寄存器 （the miss status holding register ，MSHR）已满，或者cache集中没有可替换的块，则会导致预定失败，因为所有块都是由先前挂起的请求保留的（有关详细信息，请参阅后续#缓存部分）。在命中与未命中两种情况下，轮循机制的调度程序都将转到下一个warp。如果发生命中，提取的指令将被发送到解码阶段。在未命中的情况下，指令高速缓存将生成读指令请求。当收到未命中响应时，该块被填充到指令高速缓存中，warp将需要再次访问指令高速缓存，但是当未命中被挂起时，warp不会访问指令缓存。\n如果某个warp完成执行，其所有线程都已完成执行，而没有任何未完成的存储或对本地寄存器的挂起写入，则fetch调度程序不再轮询该warp（即轮询机制跳过这个warp）。一旦线程块中的所有warp都完成并且没有挂起的操作，则认为线程块已完成执行。一旦某个Core启动时调度的所有线程块完成，则此Core被视为已完成执行。\n在解码阶段，最近获取的指令被解码并存储在I-Buffer中的相应entries中，等待发射（这一部分参阅后续#解码和发射部分）。\n2. 指令问题第二个轮询仲裁器（另外一个位于I-Buffer中的Issue ARB）选择一个warp，从I-Buffer中取指令，发射到流水线剩余部分。这个轮询仲裁器与用于调度指令缓存访问的轮询仲裁器是解耦合的（不是同一个）。可以对发射仲裁器进行配置，可以配置为在每个周期内为同一warp发射多条指令。当前检查的warp中的每条有效指令（即解码后未发出的指令）都有资格发出，如果满足（1）其warp没有在障碍处等待，（2）其I-Buffer entries中有有效指令（有效位被设置为1），（3）记分牌检查通过（有关详细信息，请参阅#记分牌部分），以及（4）指令流水线的操作数访问阶段没有停顿。\n内存指令（加载、存储或内存障碍）被发射到内存流水线（memory pipeline）中。对于其他指令，对于可以同时使用SP（流处理器）和SFU（特殊功能单元，用以处理超越函数和纹理获取插值）流水线的操作，它总是优先选择SP流水线。然而，如果检测到控制冒险，那么对应于这个warp的I-Buffer中的指令被刷新。warp的下一个PC被更新为指向下一条指令（假设所有的分支都没有命中）。有关处理控制流的详细信息，请参阅SIMT堆栈。\n在发射阶段，障碍操作被执行（通过阻止指令发射实现障碍功能）。同时，SIMT堆栈被更新（更多细节请参考#SIMT堆栈），并跟踪寄存器的依赖关系（更多细节请参考#记分牌）。Warps在发射阶段等待障碍（__syncthreads()）。\n\n3. SIMT堆栈每个warp的SIMT堆栈被用来处理单指令、多线程（SIMT）架构上的分支发散执行。由于发散会降低这些架构的效率，因此可以采用不同的技术来减少这种影响。最简单的技术之一是基于栈的重叠机制。这种技术在最早的保证重叠点上同步发散的分支，以提高SIMT架构的效率。和以前的GPGPU-Sim版本一样，GPGPU-Sim 3.x也采用了这种机制。\nSIMT堆栈的条目代表不同的发散水平。在每个发散分支，一个新的条目被推到堆栈的顶部。当warp到达其重新融合点时，堆栈顶部的条目被弹出。每个条目都存储了新分支的目标PC，紧随其后的支配者重合PC（reconvergence PC，RPC）和正在向这个分支发散的线程的活动掩码。在我们的模型中，每个warp的SIMT栈在该warp的每个指令发出后都会被更新。在没有分支的情况下，目标PC通常被更新到下一个PC。然而，在出现分支的情况下，新的条目被推送到堆栈中，其中包括新的目标PC、与分歧到该PC的线程相对应的活动掩码以及它们的直接重新汇合点PC（RPC）。因此，如果SIMT堆栈顶部条目的下一个PC不等于当前正在检查的指令的PC，就会检测到控制危险。查看Dynamic Warp Formation: Efficient MIMD Control Flow on SIMD Graphics Hardware以获得更多信息。\n请注意，众所周知，NVIDIA和AMD实际上是使用特殊指令来修改其分支堆栈的内容。这些发散堆栈指令在PTX中没有暴露，但在实际的硬件SASS指令集中是可见的（使用decuda或NVIDIA的cuobjdump可见）。当当前版本的GPGPU-Sim 3.x被配置为通过PTXPlus执行SASS时（见#PTX vs. PTXPlus），它忽略了这些低级指令，而是创建了一个可比较的控制流图来识别直接的后支配者。我们计划在GPGPU-Sim 3.x的未来版本中支持执行低级别的分支指令。\n\n4. 记分牌记分牌算法检查WAW和RAW的依赖性危险。如上所述，warp将会写入东西的寄存器在发射阶段被保留。记分牌算法以warp ID为索引。它将所需的寄存器编号存储在与warp ID相对应的条目中。保留的寄存器在写回阶段被释放。\n如上所述，在记分牌显示不存在WAW或RAW危险之前，warp的解码指令不会被安排发射。计分牌通过跟踪哪些寄存器将被已经发射但尚未将结果写回寄存器文件的指令写入来检测WAW和RAW的危险。\n（二）寄存器访问和操作数收集器英伟达的多项专利描述了一种名为 “操作数收集器 “的结构。操作数收集器是一组缓冲器和仲裁逻辑，用于提供一个实际上使用多bank单端口RAM而能够表现出多端口寄存器文件的外观。整个安排节省了能源和面积，这对提高吞吐量很重要。请注意，AMD公司也使用bank式寄存器文件，但编译器负责确保这些文件的访问不会发生bank冲突。\n下图提供了GPGPU-Sim 3.x对操作数收集器进行建模的详细方式的说明：\n\n在指令被解码后，一个叫做收集器单元的硬件单元被分配来缓冲指令的源操作数。\n收集器单元不是用来通过寄存器重命名来消除名称的依赖性，而是作为一种方法来安排寄存器操作数访问的时间，以便在一个周期内对一个bank的访问不超过一次。在图中所示的组织中，四个收集器单元中的每一个都包含三个操作数条目。每个操作数条目有四个域：一个有效位、一个寄存器标识符、一个就绪位和操作数数据。每个操作数数据字段可以容纳一个由32个四字节元素组成的128字节源操作数（warp中每个标量线程有一个四字节值）。此外，收集器单元包含一个标识符，表明该指令属于哪个warp。仲裁器包含一个每个bank的读请求队列，以保持访问请求，直到它们被批准。\n当一个指令从解码阶段收到，并且有一个收集器单元可用时，它被分配给该指令，并且操作数、warp ID、寄存器标识符和有效位被设置。此外，源操作数的读取请求在仲裁器中被排队。为了简化设计，被执行单元写回的数据总是优先于读请求。仲裁器选择一组最多四个不冲突的访问来发送至寄存器文件。为了减少Crossbar和收集器单元的面积，选择时每个收集器单元每周期只接收一个操作数。\n当每个操作数从寄存器文件中读出并放入相应的收集器单元时，一个”就绪位”被设置。最后，当所有的操作数都准备好了，指令就被发射到SIMD执行单元。\n在我们的模型中，每个后端流水线（SP、SFU、MEM）都有一组专用的收集器单元，它们共享一个通用收集器单元池。每个流水线可用的单元数量和一般单元池的容量是可配置的。\n\n（三）ALU流水线GPGPU Sim v3.x为两种类型的ALU功能单元建模：\n\nSP单元执行除超越指令外的所有类型的ALU指令。\nSFU单元执行超越指令（正弦、余弦、对数等）。\n\n这两种类型的单元都是流水线和SIMD。SP单元通常每个周期可以执行一条warp指令，而SFU单元根据指令类型，每几个周期只能执行一条新的warp指令。例如，SFU单元可以每4个周期执行一个正弦指令或每2个周期执行倒数指令。不同类型的指令也有不同的执行延迟。\n每个SIMT Core有一个SP单元和一个SFU单元。每个单元都有一个独立于操作数收集器的发出端口。两个单元共享连接到公共写回阶段的相同输出流水线寄存器。在操作数收集器的输出端有一个结果总线分配器，以确保单元不会因共享写回而停止。每一条指令都需要在结果总线中分配一个循环槽，然后才能发送给任何一个单元。请注意，内存流水线有自己的写回阶段，不受此结果总线分配器的管理。\n软件设计部分包含模型的更多实现细节。\n（四）内存流水线（LDST单元）GPGPU Sim支持CUDA中PTX中可见的各种内存空间。在我们的模型中，每个SIMT Core有4个不同的片上1级存储器：共享存储器、数据缓存、常量缓存和纹理缓存。下表显示了哪些片上存储器服务哪种类型的存储器访问：| 核心存储器 | PTX访问 ||:————–|:————–|| 共享内存（R/W）| CUDA共享内存（OpenCL本地内存）只能访问 || 常量缓存（只读）| 常量内存和参数内存 || 纹理缓存（只读）| 仅纹理访问 || 数据缓存（R/W-写入时退出全局内存，写回本地内存）| 全局和本地内存访问（本地内存=OpenCL中的私有数据）|\n尽管这些被建模为单独的物理结构，但它们都是内存流水线（LDST单元）的组件，因此它们都共享相同的写回阶段。以下介绍了这些空间的服务方式：\n\n纹理内存-对纹理内存的访问缓存在L1纹理缓存中（仅为纹理访问保留），也缓存在L2缓存中（如果启用）。L1纹理缓存是1998年论文[3]中描述的一种特殊设计。GPU上的线程无法写入纹理内存空间，因此L1纹理缓存是只读的。\n共享内存-每个SIMT Core都包含可配置数量的共享内存，可由线程块内的线程共享。此内存空间不受任何L2支持，并由程序员明确管理。\n常量内存-常量和参数内存缓存在只读常量缓存中。\n参数存储器-见上文\n本地内存-缓存在一级数据缓存中，并由二级数据缓存支持。处理方式类似于下面的全局内存，但值在逐出时被写回，因为不能共享本地（私有）数据。\n全局内存-全局和本地访问都由一级数据缓存提供服务。如CUDA 3.1编程指南中所述，来自同一warp的标量线程访问在half-warp基础上合并[4]。这些访问以每SIMT Core周期2次的速率进行处理，使得完全合并为2次访问（每half-warp一次）的内存指令可以在单个周期内得到服务。对于那些生成2次以上访问的指令，这些指令将以每个周期2次的速率访问内存系统。因此，如果一条内存指令生成32次访问（每通道一次），则至少需要16个SIMT Core周期才能将该指令移动到下一个流水线阶段。\n\n下面的小节描述了第一级内存结构。\n\n1. L1 Data CacheL1 Data Cache是用于本地和全局内存访问的私有的、每个SIMT Core都有的、无阻塞的一级缓存。L1缓存没有划分bank，并且能够为每个SIMT Core周期的两个合并内存请求提供服务。传入的内存请求不能跨越一级数据缓存中的两个或多个Cache lines。还请注意，L1数据缓存不是一致性的。\n下表总结了一级数据缓存的写入策略：| L1 data cache 写策略  | | ||:————–|:————–|:————–||  | Local Memory | Global Memory || Write Hit | Write-back（写回） | Write-evict（写入逐出） || Write Miss | Write no-allocate（写不分配） | Write no-allocate（写不分配） |\n对于本地内存，一级数据缓存充当写不分配的写回缓存。对于全局内存，写命中会导致块被逐出。这模仿了PTX ISA规范[5]中概述的全局存储的默认策略。\n命中一级数据缓存的内存访问在一个SIMT Core时钟周期内提供服务。未命中的访问被插入FIFO未命中队列。每个SIMT时钟周期一个填充请求由L1数据缓存生成（假定互连注入缓冲区能够接受该请求）。\n缓存使用未命中状态保持寄存器（Miss Status Holding Registers，MSHR）来保持正在进行的未命中状态。它们被建模为完全关联数组。当一个请求正在运行时，对内存系统的冗余访问被合并到MSHR中。MSHR表具有固定数量的MSHR条目。每个MSHR条目可以为单个缓存行（Cache Line）提供固定数量的未命中请求。MSHR条目的数量和每个条目的最大请求数是可配置的。\n缓存中未命中的内存请求被添加到MSHR表，如果该缓存行没有挂起的请求，则生成填充请求。当在缓存处接收到对填充请求的填充响应时，将缓存行插入缓存中，并将相应的MSHR条目标记为已填充。每个周期一个请求生成一个已填充MSHR条目的响应。一旦在填充的MSHR条目处等待的所有请求都得到响应和服务，MSHR条目将被释放。\n2. 纹理 Cache纹理缓存模型是预取纹理缓存。纹理存储器访问主要表现出空间局部性，并且已经发现这种局部性主要用大约16KB的存储来捕获。在现实的图形使用场景中，许多纹理缓存访问失败。在DRAM中访问纹理的延迟大约为100个周期。考虑到大的内存访问延迟和小的缓存大小，何时在缓存中分配行的问题变得至关重要。预取纹理缓存通过在时间上将缓存标签的状态与缓存块的状态解耦来解决该问题。标记数组表示缓存在100个周期后处理未命中时所处的状态。数据数组表示未命中得到处理后的状态。使这种去耦工作的关键是使用重新排序缓冲区，以确保返回的纹理缺失数据以标记数组看到访问的相同顺序放置到数据数组中。有关详细信息，请参阅原始论文。\n3. 常量（只读） Cache通过L1常量缓存访问常量和参数存储器。此缓存使用标记数组实现，与L1数据缓存类似，但无法写入。\n（五）线程块/CTA/Warp组调度线程块，CUDA术语中的协作线程阵列（Cooperative Thread Arrays，CTA）或OpenCL术语中的warp组（Work Groups），一次一个地发给SIMT Core。每一个SIMT Core时钟周期，线程块发布机制都会以周期方式选择并循环SIMT Core集群。对于每个选定的SIMT Core集群，将选择SIMT Core并以周期方式循环。对于每个选定的SIMT Core，如果该SIMT Core上有足够的可用资源，则将从选定的kernel向该Core发出单个线程块。\n如果应用程序中使用了多个CUDA流或命令队列，则可以在GPGPU-Sim中同时执行多个kernel。不同的kernel可以跨不同的SIMT Core执行；单个SIMT Core一次只能执行来自单个kernel的线程块。如果同时执行多个kernel，那么选择要发布给每个SIMT Core的kernel也是循环的。《NVIDIA CUDA编程指南》中描述了CUDA架构上的并发内核执行。\n四、互连网络部分互连网络负责SIMT Core集群和内存分区单元之间的通信。为了模拟互连网络，我们将“booksim”模拟器连接到GPGPU-Sim。Booksim是一个独立的网络模拟器，可以在这里找到。Booksim能够模拟基于Tori和Fly网络的虚拟信道，并且高度可配置。可以通过参考Dally和Towles的《互联网络的原理和实践》一书来更好地理解这一点。\n我们将修改后的booksim称为Intersim。Intersim有自己的时钟域。最初的booksim只支持单个互连网络。我们做了一些更改，以便能够模拟两个互连网络：一个用于从SIMT Core集群到内存分区的流量，另一个网络用于从内存分区返回到SIMT Core群集的流量。这是避免可能导致系统中协议死锁的循环依赖关系的一种方法。另一种方式是在单个物理网络上为请求和响应流量提供专用虚拟通道，但我们的公共发布的当前版本不完全支持此功能。注意：现在斯坦福大学提供了更新版本的Booksim（Booksim 2.0），但GPGPU Sim 3.x尚未使用。\n请注意，SIMT Core集群之间不直接通信，因此互连网络中没有一致性流量的概念。只有四种数据包类型：（1）从SIMT Core集群向内存分区发送的读请求和写请求，以及（3）从内存分区向SIMT Core群集发送的读应答和写确认。\n注意：SIMT Core集群在GPGPU Sim中充当外部集中器。从互连网络的角度来看，SIMT Core集群是单个节点，连接到该节点的路由器只有一个注入端口和一个排出端口。\n与GPGPU Sim的接口：互连网络无论其内部配置如何，都提供了一个简单的接口来与连接到它的SIMT Core集群和内存分区进行通信。对于注入数据包，SIMT Core集群或内存控制器首先检查网络是否有足够的缓冲区空间来接受其数据包，然后将其数据包发送到网络。对于弹出，他们检查网络中是否有等待弹出的数据包，然后将其弹出。这些动作发生在每个单元的时钟域中。数据包的串行化在网络接口内处理，例如，SIMT Core集群在SIMT Core群集时钟域中注入数据包，但路由器每个互连时钟周期仅接受一个微片。更多的实现细节可以在软件设计部分找到。\n\n五、内存分区GPGPU-Sim中的内存系统由一组内存分区建模。如下图所示，每个内存分区包含一个L2缓存bank、一个DRAM访问调度器和片外DRAM Channel。原子操作的功能执行也发生在原子操作执行阶段的内存分区中。每个内存分区都分配了一个子组物理内存地址，由它负责。默认情况下，全局线性地址空间以256字节的块在分区之间交错。地址空间的这种划分以及地址空间到每个分区中DRAM行、bank和列的详细映射是可配置的，并在地址解码部分中描述。\n\nL2缓存（如果启用时）为传入纹理和（如果配置时）非纹理内存请求提供服务。请注意，Quadro FX 5800（GT200）配置仅为纹理参考启用L2。在缓存未命中时，二级缓存组向DRAM Channel生成内存请求，以由片外GDDR3 DRAM提供服务。\n下面的小节更详细地描述了流量如何通过内存分区以及上面提到的各个组件。\n\n（一）内存分区连接和流量上面的图显示了单个内存分区内的三个子组件和各种FIFO队列，这些队列促进了内存请求和响应在它们之间的流动。\n内存请求数据包通过ICNT-&gt;L2 queue从互连网络进入内存分区。如GT200微基准测试研究所观察到的，非纹理访问通过光栅操作流水线（Raster Operations Pipeline，ROP）队列进行，以模拟460 L2时钟周期的最小流水线延迟。L2 Cache Bank在每个L2时钟周期从ICNT-&gt;L2 queue弹出一个请求进行服务。L2生成的芯片外DRAM的任何内存请求都被推入L2-&gt;DRAM queue。如果L2 Cache被禁用，数据包将从ICNT-&gt;L2 queue弹出，并直接推入L2-&gt;DRAM queue，仍然以L2时钟频率。从片外DRAM返回的填充请求从DRAM-&gt;L2 queue弹出，并由L2 Cache Bank消耗。从L2到SIMT Core的读响应通过L2-&gt;ICNT queue推送。\nDRAM latency queue是一个固定延迟队列，它模拟了L2访问和DRAM访问（错过了L2缓存的访问）之间的最小延迟差。这种延迟是通过微基准测试观察到的，这个队列只是对这种观察进行建模（而不是导致这种延迟的真正硬件）。L2-&gt;DRAM queue的请求在DRAM延迟队列中驻留固定数量的SIMT Core时钟周期。每个DRAM时钟周期，DRAM Channel可以从DRAM延迟队列中弹出内存请求，由片外DRAM提供服务，并将一个已服务的内存请求推入DRAM-&gt;L2 queue。\n注意，从互连网络到内存分区（ROP或ICNT-&gt;L2 queue）的弹出发生在L2时钟域中，而从存储器分区（L2-&gt;ICNT queue）到互连的注入发生在互连（ICNT）时钟域中。\n（二）L2 Cache模型和缓存层次结构二级缓存模型与SIMT Core中的L1 Data Cache非常相似（有关详细信息，请参阅#L1 Data Cache部分）。当启用缓存全局内存空间数据时，L2充当读/写缓存，其写入策略如下表所示：\n\n\n\nL2 cache 写策略\n\n\n\n\n\n\nLocal Memory\nGlobal Memory\n\n\nWrite Hit\nWrite-back for L1 write-backs（L1写回的写命中-写回）\nWrite-evict（写入逐出）\n\n\nWrite Miss\nWrite no-allocate（写不分配）\nWrite no-allocate（写不分配）\n\n\n此外，请注意，L2缓存是由所有SIMT Core共享的统一最后一级缓存，而L1缓存是每个SIMT Core的私有缓存。\n私有L1 Data Cache不一致（其他L1缓存用于只读地址空间）。GPGPU-Sim中的缓存层次结构是非独占的。此外，强制执行沿缓存层次结构向下的非减小缓存行大小（即，增加缓存级别）。来自一级缓存的内存请求也不能跨越二级缓存中的两条Cache Line。这两个限制确保：\n\n来自较低级别缓存的请求可以由较高级别缓存中的一个缓存行提供服务。这确保了来自L1的请求可以由L2缓存原子地服务。\n原子操作不需要访问L2上的多个块。\n\n这一限制简化了缓存设计，并避免了在非原子的服务L1请求时必须处理与实时锁相关的问题。\n（三）原子操作执行阶段原子操作执行阶段是原子指令执行的一个非常理想化的模型。具有非冲突内存访问的原子指令合并为一个内存请求，在一个周期内在内存分区执行。在性能模型中，我们当前将原子操作建模为跳过L1数据缓存的全局加载操作。这将在SIMT Core内生成所有必要的寄存器写回流量（和数据冒险时的停顿）。在二级缓存中，原子操作将访问的缓存线标记为脏（将其状态更改为已修改），以生成到DRAM的写回流量。如果未启用二级缓存（或仅用于纹理访问），则不会为原子操作生成DRAM写入流量（非常理想的模型）。\n（四）DRAM调度与时序模型GPGPU-Sim对DRAM调度和时序进行建模。GPGPU-Sim实现了两个开放页面模式DRAM调度器：一个FIFO（先进先出）调度器和一个FR-FCFS（First-Row First-Come-First-Served，先准备先到先服务）调度器，这两个调度器都在下面描述。可以使用配置选项-gpgpu_dram_scheduler选择这些选项。\n1. FIFO调度程序：FIFO调度器按照接收请求的顺序为请求提供服务。这将导致大量的precharges和activates，并因此导致较差的性能，特别是对于生成与执行的计算量相关的大量内存流量的应用程序。\n2. FR-FCFS调度程序：FR-FCFS调度程序对任何DRAM bank中当前打开的行的请求给予更高的优先级。调度程序将调度队列中的所有请求首先执行首先打开行。如果不存在这样的请求，它将为最旧的请求打开一个新行。此调度程序的代码位于dram_sched.h/.cc中。\n3. DRAM时序模型GPGPU-Sim精确模拟DRAM内存。目前GPGPU Sim 3.x型号为GDDR3 DRAM，但我们正在努力添加详细的GDDR5。可以使用选项-ggpu_DRAM_timing_opt nbk:tCD:tRRD:tRCD:tRAS：tRP:tRC：CL:WL:TCDR:tWR设置以下DRAM定时参数。目前，我们没有对DRAM刷新操作的时序进行建模。有关每个参数的详细信息，请参阅GDDR3规范。\n\nnbk：number of banks\ntCCD：Column to Column Delay（RD/WR to RD/WR different banks）\ntRRD：Row to Row Delay（Active to Active different banks）\ntRCD：Row to Column Delay（Active to RD/WR/RTR/WTR/LTR）\nActive to PRECHARGE command period\ntRP: PRECHARGE command period\ntRC: Active to Active command period (same bank)\nCL: CAS Latency\nWL: WRITE latency\ntCDLR: Last data-in to Read Delay (switching from write to read)\ntWR: WRITE recovery time \n\n在我们的模型中，每个Bank的命令都是以循环方式调度的。Bank以圆形阵列排列，指针指向具有最高优先级的Bank。调度程序按顺序遍历Bank并发出命令。每当为一个Bank发出激活或预充电命令时，将优先级指针设置为下一个Bank，以保证其他Bank的其他挂起命令最终将被调度。\n六、指令集（一）PTX和SASSGPGPU-Sim模拟NVIDIA使用的并行线程执行（Parallel Thread eXecution，PTX）指令集。PTX是伪汇编指令集；它不直接在硬件上执行。ptxas是NVIDIA发布的汇编程序，用于将PTX汇编到硬件运行的本机指令集（SASS）中。每一代硬件都支持不同版本的SASS。因此，PTX被编译成多个版本的SASS，对应于编译时不同的硬件版本。尽管如此，PTX代码仍然嵌入到二进制文件中，以支持未来的硬件。在运行时，运行时系统根据可用硬件选择要运行的SASS的适当版本。如果没有，则运行时系统调用嵌入式PTX上的实时（JIT）编译器，将其编译为与可用硬件相对应的SASS。\n（二）PTXPlusGPGPU Sim能够运行PTX。然而，由于PTX不是在硬件上运行的实际代码，因此它的准确程度有限。这主要是由于编译器传递，例如强度降低、指令调度、寄存器分配等等。\n要在GPGPU Sim中运行SASS代码，必须添加新功能：\n\n新的寻址模式\n更详细的条件代码和谓词\n额外指令\n其他数据类型\n\n为了避免开发和维护两个解析器和两个功能执行引擎（一个用于PTX，另一个用于SASS），我们选择使用所需的功能扩展PTX，以提供到SASS的一对一映射。PTX及其扩展称为PTXPlus。为了运行SASS，我们执行从SASS到PTXPlus的语法转换。\n与PTX相比，PTXPlus具有非常相似的语法，添加了新的寻址模式、更复杂的条件代码和谓词、额外的指令和更多的数据类型。必须记住，PTXPlus是PTX的超集，这意味着有效的PTX也是有效的PTXPlus。有关PTX和PTXPlus之间的确切差异的更多详细信息，请参见#PTX vs. PTXPlus。\n（三）从SASS到PTXPlus当配置文件指示GPGPU-Sim运行SASS时，使用转换工具cuobjdump_to_ptxblus将嵌入二进制文件中的SASS转换为PTXPlus。有关转换过程的完整详细信息，请参阅#PTXPlus转换。然后在仿真中使用PTXPlus。当SASS转换为PTXPlus时，只更改语法，指令及其顺序与SASS完全相同。因此，完全捕获了应用于本机代码的编译器优化的效果。目前，GPGPU-Sim仅支持GT200 SASS到PTXPlus的转换。\n第三部分：如何使用GPGPU-SIM有关构建和运行GPGPU-Sim 3.x的说明，请参阅顶层GPGPU-Sim目录中的README文件。本节提供了有关使用GPGPU-Sim 3.x的其他重要指导，包括不同模拟模式、如何修改时序模型配置、默认模拟统计的说明，以及通过跟踪模拟状态和类似GDB的接口在功能级别分析bug的方法的描述。GPGPU Sim 3.x还为调试性能模拟错误提供了广泛的支持，包括高级微体系结构可视化程序和逐周期流水线状态可视化。接下来，我们将描述在性能模拟模式下GPGPU-Sim崩溃或死锁时的调试策略。最后，它以常见问题的答案作为结尾。\n\n一、模拟模式默认情况下，大多数用户都希望使用GPGPU Sim 3.x来估计运行应用程序所需的GPU时钟周期数。这就是所谓的性能模拟模式。当尝试在GPGPU-Sim上运行新应用程序时，总是有可能应用程序无法正确运行——即可能会生成错误的输出。为了帮助调试此类应用程序，GPGPU Sim 3.x还支持快速功能模拟模式。此模式也可能有助于编译器研究和/或对功能模拟引擎进行更改。与性能和功能模拟之间的区别正交，GPGPU Sim 3.x还支持在NVIDIA GPU（当前仅为GT200和更早版本）上执行本地硬件ISA，通过我们称为PTXPlus的扩展PTX语法。以下各小节依次介绍这些功能。\n下表展示了GPGPU Sim的操作模式：| CUDA Version |    PTX | PTXPlus |    cuobjdump+PTX |    cuobjdump+PTXPlus ||:————-|:—–|:——–|:————–|:——————|| $2.3$ | $?$ | $\\color{red}{No}$ | $\\color{red}{No}$ | $\\color{red}{No}$ || $3.1$ |    $\\color{green}{Yes}$ | $\\color{red}{No}$ | $\\color{red}{No}$ | $\\color{red}{No}$ || $4.0$ |    $\\color{red}{No}$ | $\\color{red}{No}$| $\\color{green}{Yes}$ |    $\\color{green}{Yes}$ |\n（一）性能模拟性能模拟是模拟的默认模式，以较慢的模拟速度来收集性能统计信息，该信息来自于GPGPU-Sim模拟微体系结构模型部分中描述的微体系结构。\n要选择性能模拟模式，请将以下行添加到gpgpusim.config文件：\n-gpgpu_ptx_sim_mode 0\n\n有关了解模拟输出的信息，请参阅了解#模拟输出一节。\n（二）纯功能模拟纯功能模拟比性能模拟运行得更快，但仅执行CUDA/OpenCL程序的执行，不收集性能统计信息。\n要选择纯功能模拟模式，请在gpgpusim.config中添加以下行：\n-gpgpu_ptx_sim_mode 1\n\n或者，您可以将环境变量PTX_SIM_MODE_FUNC设置为1。然后像在性能模拟模式下一样正常执行程序。\n仅模拟GPU设备的功能，GPGPU-Sim纯功能模拟模式执行CUDA/OpenCL程序，就像它在真实的GPU设备上运行一样，因此在此模式下不会收集性能度量，只显示GPU程序的常规输出。正如您所期望的那样，纯模拟模式明显快于性能模拟模式（大约快5~10倍）。\n如果您想快速检查代码是否在GPGPU-Sim上正常工作，或者您想体验使用CUDA/OpenCL而不需要真正的GPU计算设备，则此模式非常有用。纯功能模拟支持与PTX Plus性能模拟（CUDA v3.1）和（CUDA v2.3）相同版本的CUDA。纯功能模拟模式将程序作为一组warp执行，其中每个协同线程阵列（CTA）的warp被执行，直到它们全部完成或全部在屏障处等待，在后一种情况下，一旦所有warp在屏障处相遇，它们就可以继续前进并越过屏障。\n纯功能模拟的软件设计细节见下文。\n\n（三）交互式调试器模式交互式调试器模式提供了一个类似GDB的界面，用于调试GPGPU-Sim中的功能行为。然而，目前它只适用于性能模拟。\n要启用交互式调试器模式，请将环境变量GPGPUSIM_DEBUG设置为1。以下是支持的命令：\n\n\n\n命令\n描述\n\n\n\ndp [id]\nDump pipeline: Display the state (pipeline content) of the SIMT core [id].\n\n\nq\nQuit\n\n\nb [file]:[line] [thread uid]\nSet breakpoint at [file]:[line] for thread with [thread uid].\n\n\nd [uid]\nDelete breakpoint.\n\n\ns\nSingle step execution to next core cycle for all cores.\n\n\nc\nContinue execution without single stepping.\n\n\nw [address]\nSet watchpoint at [address]. [address] is specified as a hexadecimal number.\n\n\nl\nList PTX around current breakpoint.\n\n\nh\nDisplay help message.\n\n\n上述命令是在debug.h和debug.cc实现的。\n（四）Cuobjdump支持从GPGPU Sim 3.1.0版开始，添加了对使用cuobjdump的支持。cuobjdump是Nvidia提供的软件，用于从二进制文件中提取SASS和PTX等信息。GPGPU-Sim支持使用cuobjdump提取运行SASS或PTX所需的信息，而不是通过cubin文件获取这些信息。只有CUDA 4.0支持使用cuobjdump。如果使用CUDA 4.0编译模拟器，则默认情况下启用cuobjdmp。要启用/禁用cuobjddump，请在配置文件中添加以下选项之一：\n# disable cuobjdump-gpgpu_ptx_use_cuobjdump 0\n# enable cuobjdump-gpgpu_ptx_use_cuobjdump 1\n\n\n（五）PTX vs. PTXPlus默认情况下，GPGPU Sim 3.x模拟PTX指令。然而，当在实际GPU上执行时，PTX被重新编译为本地GPU ISA（SASS）。在模拟正常PTX指令时，未充分考虑这种重新编译。为了解决这个问题，我们创建了PTXPlus。PTXPlus是GPGPU Sim 3.x引入的PTX的一种扩展形式，它允许大多数GT200 SASS指令到PTXPlus指令的接近1到1的映射。它包括常规PTX中不存在的新指令和寻址模式。当激活转换为PTXPlus选项时，组成程序的SASS指令被转换为可由GPGPU-Sim模拟的PTXPlus指令。使用PTXPlus转换选项可以获得更准确的结果。然而，转换到PTXPlus还不能完全支持所有可以使用普通PTX模拟的程序。目前，仅支持4.0之后的CUDA Toolkit转换为PTXPlus。\nSASS是NVIDIA根据GPU发布的指令集文档所使用的本机指令集的术语。该文档可以在CUDA工具包发布的文件cuobjdump.pdf中找到。\n为了将SASS从可执行文件转换为GPGPU Sim cuobjdump，这是NVIDIA发布的CUDA工具包的一个软件版本，它从CUDA可执行文件中提取PTX、SASS和其他信息。GPGPU Sim 3.x包括一个名为cuobjdump_to_ptxblus的独立程序，该程序被调用以将cuobjdmp的输出转换为GPGPU Sim可以模拟的ptxplus。cuobjdump_to_ptxblus是一个用C++编写的程序，其源代码随GPGPU-Sim发行版提供。有关PTXPlus转换过程的详细说明，请参阅#PTXPlus转换部分。目前，cuobjdump_to_ptxblus支持sm版本 &lt; sm_20的SASS转换。\n要启用PTXPlus模拟，请将以下行添加到gpgpusim.config文件：\n-gpgpu_ptx_convert_to_ptxplus 1\n\n此外，通过向gpgpusim.config添加以下行，可以将转换后的PTXPlus保存到名为_#.ptplus的文件中。配置文件：\n-gpgpu_ptx_save_converted_pxplus 1\n\n要关闭任一功能，请删除该行或将值从1更改为0。有关使用PTXPlus的更多详细信息，请参阅#PTXPlus支持。如果启用了上述选项，GPGPU-Sim将尝试将SASS代码转换为PTXPlus，然后运行生成的PTXPlus。但是，如前所述，并非所有程序都支持此模式。\n下面的小节描述了我们为获得PTXPlus而对PTX进行的添加。\n1. 寻址模式为了支持GT200 SASS，PTXPlus增加了大多数指令可用的寻址模式的数量。非加载/非存储现在可以直接访问内存。以下指令将寄存器r0中的值添加到共享存储器地址0x0010处的值存储中，并将值存储在寄存器r1中：\nadd.u32 $r1, s[0x0010], $r0;\n\n也可以使用诸如s[$r2]或s[$ofs1+0x0010]之类的操作数。PTXPlus还引入了原始PTX中不存在的以下寻址模式：\n\ng=全局内存\ns=共享内存\nce#c#=常量内存（第一个数字是内核号，第二个数字是常量段）g[$ofs1+$r0]        //由寄存器ofs1和寄存器r0之和确定的全局内存地址。s[$ofs1+=0x0010]    //由寄存器$ofs1中的值确定的共享内存地址。然后，寄存器$ofs2递增0x0010。ce1c2[$ofs1+=$r1]   //第一个内核的第二个常量段的内存地址由寄存器$ofs1中的值确定。然后，寄存器$ofs2将按寄存器$r1中的值递增。\n\nPTXPlus实现中描述了这些寻址模式的实现细节。\n2. 新增数据类型指令也已升级，以更准确地表示64位和128位值如何跨多个32位寄存器存储。最低有效32位存储在最左边的寄存器中，而最高有效32位则存储在最右边的寄存器中。以下是新数据类型的列表以及添加两个64位浮点数的加法指令的示例：\n\n.ff64=64位浮点数的PTXPlus版本\n.bb64=64位非类型化的PTXPlus版本\n.bb128=128位非类型化的PTXPlus版本add.rn.ff64 &#123;$r0,$r1&#125;, &#123;$r2,$r3&#125;, &#123;$r4,$r5&#125;;\n\n3. PTXPlus指令\n\n\nPTXPlus Instructions\n\n\n\n\nnop\nDo nothing\n\n\nandn\na andn b = a and ~b\n\n\nnorn\na norn b = a nor ~b\n\n\norn\na orn b = a or ~b\n\n\nnandn\na nandn b = a nand ~b\n\n\ncallp\nA new call instruction added in PTXPlus. It jumps to the indicated label\n\n\nretp\nA new return instruction added in PTXPlus. It jumps back to the instruction after the previous callp instruction\n\n\nbreakaddr\nPushes the address indicated by the operand on the thread’s break address stack\n\n\nbreak\nJumps to the address at the top of the thread’s break address stack and pops off the entry\n\n\n4. PTXPlus条件码和指令预测SASS指令使用4位条件代码来指定更复杂的谓词行为，而不是PTX中的正常真假谓词系统。因此，PTXPlus使用相同的4位谓词系统。GPGPU-Sim使用decuda的谓词转换表来模拟PTXPlus指令。\n最高位表示溢出标志，后面是进位标志和符号标志。最后也是最低的位是零标志。单独的条件代码可以存储在单独的谓词寄存器中，指令可以指示要使用或修改哪个谓词寄存器。以下指令将寄存器$r0中的值与寄存器$r1中的值相加，并将结果存储在寄存器$r2中。同时，在谓词寄存器$p0中设置适当的标志。\nadd.u32 $p0|$r2, $r0, $r1;\n\n可以对预测指令使用不同的测试条件。例如，只有当谓词寄存器$p0中的进位标志位被设置时，才执行下一条指令：\n@$p0.cf add.u32 $r2, $r0, $r1;\n\n5. 参数和线程ID（tid）初始化PTXPlus不使用显式参数状态空间来存储内核参数。相反，输入参数从地址0x0010开始按顺序复制到共享内存中。参数的复制在GPGPU-Sim的线程初始化过程中执行。线程初始化过程发生在向SIMT Core发出线程块时，如线程块/CTA/warp group调度中所述。内核启动：参数连接部分描述了此过程的实现。此外，在此过程中，特殊寄存器%tid.x、%tid.y和%tid.z被复制到寄存器$r0中。\nRegister $r0:|%tid.z|  %tid.y  |  NA  |  %tid.x  ||31  26|25      16|15  10|9        0|\n\n二、通过Prints和Traces进行调试有两个内置的调试gpu-sim的工具。第一种机制是通过环境变量。这对于在解析配置文件（gpgpusim.config）之前调试GPGPU-Sim的元素非常有用，但是这可能是在性能模拟模式中实现跟踪信息的笨拙方式。从3.2.1版开始，GPGPU-Sim包括一个在src/trace.h中实现的跟踪系统，它允许用户通过配置文件打开和关闭跟踪，并通过字符串名称启用跟踪。下面将描述这两种系统。请注意，许多环境变量打印可以通过跟踪系统实现，但由于它们在遗留代码中，所以作为env变量存在。此外，GPGPU-Sim打印大量信息，这些信息不是通过跟踪系统控制的，这也是遗留代码的结果。\n\n（一）用于调试的环境变量GPGPU Sim 3.x的一些与调试相关的行为可以通过环境变量进行配置。\n调试时，生成有关模拟器中正在发生的情况的附加信息并将其打印到标准输出可能会有所帮助。这是通过使用以下环境变量完成的：\nexport PTX_SIM_DEBUG=&lt;#&gt; enable debugging and set the verbose level\n当前支持的Level列举如下：| Level | 描述 ||:—-|:—- || 1   | Verbose logs for CTA allocation || 2   | Print verbose output from dominator analysis || 3   | Verbose logs for GPU malloc/memcpy/memset || 5   | Display the instruction executed || 6   | Display the modified register(s) by each executed instruction || 10  | Display the entire register file of the thread executing the instruction|| 50  | Print verbose output from control flow analysis || 100 | Print the loaded PTX files |\n如果基准测试在GPGPU-Sim上运行不正确，您可能需要调试功能模拟引擎。我们这样做的方式是打印出生成错误输出的单个线程的功能状态。要启用打印单个线程的功能模拟状态，请使用以下环境变量（并为PTX_SIM_DEBUG设置适当的级别）：\nexport PTX_SIM_DEBUG_THREAD_UID=&lt;#&gt; ID of thread to debug\n其他环境配置选项：\nexport PTX_SIM_USE_PTX_FILE=&lt;non-empty-string&gt; override PTX embedded in the binary and revert to old strategy of looking for *.ptx files (good for hand-tweaking PTX)export PTX_SIM_KERNELFILE=&lt;filename&gt; use this to specify the name of the PTX file\n\n（二）GPGPU-Sim debug追踪跟踪系统由gpgpusim.config中的变量控制：| 变量 | 值 | 描述 ||:—-|:—-|:—-||trace_enabled| 0 or 1 |全局启用或禁用所有跟踪。如果启用，则打印trace_components。||trace_components| &lt;WARP_SCHEDULER,SCOREBOARD,…&gt; |要启用的跟踪元素的逗号分隔列表，完整列表可在src/trace_streams.tup中找到。||trace_sampling_core| &lt;0 through num_cores-1&gt; |对于与给定shader core关联的元素（如warp调度器或记分牌），仅打印来自该核心的轨迹。|\n实施该系统的代码文件包括：| 变量 | 描述 ||:—-|:—-||src/trace_streams.tup|列出每个打印流的名称||src/trace.cc|一些设置实现和初始化||src/trace.h|定义追踪系统的所有高级接口||src/gpgpu-sim/shader_trace.h|定义一些用于调试特定shader core的方便打印|\n\n三、配置选项配置选项通过gpgpusim.config传递到GPGPU-Sim和互连配置文件（在gpgpusim.config中使用选项-inter_config_file指定）。GPGPU Sim 3.0.2附带了NVIDIA GT200（configs/QuadroFX5800/）和费米（configs/Fermi/）的configs目录中的校准配置文件。\n以下是配置选项列表：| 仿真运行配置 |  ||:—|:—|| 选项 | 描述 ||-gpgpu_max_cycle &lt;# cycles&gt;|在达到最大周期数后尽早终止GPU模拟(0 = no limit)||-gpgpu_max_insn &lt;# insns&gt;|在达到最大指令数后尽早终止GPU模拟(0 = no limit)||-gpgpu_ptx_sim_mode &lt;0=performance (default), 1=functional&gt;|在性能或功能模拟之间进行选择（请注意，功能模拟可能会错误地模拟某些ptx代码，这些代码需要warp的每个元素在lock-step中执行）||-gpgpu_deadlock_detect &lt;0=off, 1=on (default)&gt;|在死锁时停止模拟||-gpgpu_max_cta|在达到最大CTA数后尽早终止GPU模拟(0 = no limit)||-gpgpu_max_concurrent_kernel|可以在GPU上同时运行的最大内核数|| 统计信息收集选项 |  || 选项 | 描述 ||-gpgpu_ptx_instruction_classification &lt;0=off, 1=on (default)&gt;|启用指令分类||-gpgpu_runtime_stat &lt;frequency&gt;:&lt;flag&gt;|显示运行时统计信息||-gpgpu_memlatency_stat|收集内存延迟统计信息（0x2启用MC，0x4启用队列日志）||-visualizer_enabled &lt;0=off, 1=on (default)&gt;|打开可视化工具输出（使用AerialVision可视化工具绘制保存在日志中的数据）||-visualizer_outputfile &lt;filename&gt;|指定可视化工具的输出日志文件。自动生成的文件名设置为NULL（默认情况下为“Done”）。||-visualizer_zlevel &lt;compression level&gt;|可视化工具输出日志的压缩级别（0=无压缩，9=最大压缩）||-save_embedded_ptx|将二进制文件中嵌入的ptx文件保存为&lt;n&gt;.ptx||-enable_ptx_file_line_stats &lt;0=off, 1=on (default)&gt;|打开ptx的source line统计分析||-ptx_line_stats_filename &lt;output file name&gt;|ptx的source line统计的输出文件||-gpgpu_warpdistro_shader|指定从哪个shader core收集warp大小分布||-gpgpu_cflog_interval|控制流记录器中每个快照之间的间隔||-keep|在与外部程序接口时保留GPGPU-Sim创建的中间文件||高级体系结构配置（有关建模内容的详细信息，参阅ISPASS论文）|  || 选项 | 描述 ||-gpgpu_n_mem &lt;# memory controller&gt;|此配置中的内存控制器（DRAM Channel）数量。修改此选项之前，请阅读#拓扑配置。||-gpgpu_clock_domains &lt;Core Clock&gt;:&lt;Interconnect Clock&gt;:&lt;L2 Clock&gt;:&lt;DRAM Clock&gt;|MhZ中的时钟域频率（请参阅#时钟域配置）||-gpgpu_n_clusters|处理集群的数量||-gpgpu_n_cores_per_cluster|每个群集的SIMD核数||额外体系结构配置|  || 选项 | 描述 ||-gpgpu_n_cluster_ejection_buffer_size|弹出缓冲区中的数据包数||-gpgpu_n_ldst_response_buffer_size|LD/ST单元弹出缓冲区中的响应包数||-gpgpu_coalesce_arch|合并拱门（Coalescing arch）（默认值=13，其他任何功能暂时关闭）||调度器|  || 选项 | 描述 ||-gpgpu_num_sched_per_core|每个核心warp调度器数||-gpgpu_max_insn_issue_per_warp|调度器在一个周期内每个warp可以发出的最大指令数||Shader Core Pipeline配置|  || 选项 | 描述 ||-gpgpu_shader_core_pipeline &lt;# thread/shader core&gt;:&lt;warp size&gt;:&lt;pipeline SIMD width&gt;|Shader Core Pipeline配置||-gpgpu_shader_registers &lt;# registers/shader core, default=8192&gt;|每个shader core核心的寄存器数。并发CTA的限制数量。||-gpgpu_shader_cta &lt;# CTA/shader core, default=8&gt;|shader core中并发cta的最大数量||-gpgpu_simd_model &lt;1=immediate post-dominator, others are not supported for now&gt;|SIMD分支分歧处理策略||-ptx_opcode_latency_int/fp/dp &lt;ADD,MAX,MUL,MAD,DIV&gt;|操作码延迟||-ptx_opcode_initiation_int/fp/dp &lt;ADD,MAX,MUL,MAD,DIV&gt;|操作码初始化周期。对于这个周期数，ALU的输入保持恒定。ALU在此期间不能使用新值。即如果该值为4，则意味着该单元可以每4个周期消耗一次新值。||内存子系统配置|  || 选项 | 描述 ||-gpgpu_perfect_mem &lt;0=off (default), 1=on&gt;|启用完美内存模式（零内存延迟，缓存全部命中）||-gpgpu_tex_cache:l1 &lt;nsets&gt;:&lt;bsize&gt;:&lt;assoc&gt;,&lt;rep&gt;:&lt;wr&gt;:&lt;alloc&gt;:&lt;wr_alloc&gt;,&lt;mshr&gt;:&lt;N&gt;:&lt;merge&gt;,&lt;mq&gt;:&lt;fifo_entry&gt;|纹理缓存（只读）配置。逐出策略：L=LRU，F=FIFO，R=随机||-gpgpu_const_cache:l1 &lt;nsets&gt;:&lt;bsize&gt;:&lt;assoc&gt;,&lt;rep&gt;:&lt;wr&gt;:&lt;alloc&gt;:&lt;wr_alloc&gt;,&lt;mshr&gt;:&lt;N&gt;:&lt;merge&gt;,&lt;mq&gt;|常量缓存（只读）配置。逐出策略：L=LRU，F=FIFO，R=随机||-gpgpu_cache:il1 &lt;nsets&gt;:&lt;bsize&gt;:&lt;assoc&gt;,&lt;rep&gt;:&lt;wr&gt;:&lt;alloc&gt;:&lt;wr_alloc&gt;,&lt;mshr&gt;:&lt;N&gt;:&lt;merge&gt;,&lt;mq&gt;|shader L1指令缓存（用于全局和本地内存）配置。逐出策略：L=LRU，F=FIFO，R=随机||-gpgpu_cache:dl1 &lt;nsets&gt;:&lt;bsize&gt;:&lt;assoc&gt;,&lt;rep&gt;:&lt;wr&gt;:&lt;alloc&gt;:&lt;wr_alloc&gt;,&lt;mshr&gt;:&lt;N&gt;:&lt;merge&gt;,&lt;mq&gt; （如果没有DL1，则设置为none）|L1数据缓存（用于全局和本地存储器）配置。逐出策略：L=LRU，F=FIFO，R=随机||-gpgpu_cache:dl2 &lt;nsets&gt;:&lt;bsize&gt;:&lt;assoc&gt;,&lt;rep&gt;:&lt;wr&gt;:&lt;alloc&gt;:&lt;wr_alloc&gt;,&lt;mshr&gt;:&lt;N&gt;:&lt;merge&gt;,&lt;mq&gt;|统一的分Bank的二级数据缓存配置。这指定了其中一个内存分区中的二级缓存组的配置。二级缓存总容量=xxx&lt;# memory controller&gt;||-gpgpu_shmem_size &lt;shared memory size, default=16kB&gt;|每个SIMT Core（也称为Shader Core）的共享内存大小||-gpgpu_shmem_warp_parts|共享内存冲突检查将warp划分为的部分数（Number of portions a warp is divided into for shared memory bank conflict check）||-gpgpu_flush_cache &lt;0=off (default), 1=on&gt;|在每个内核调用结束时刷新缓存||-gpgpu_local_mem_map|从本地内存空间地址映射到模拟GPU物理地址空间(default = enabled)||-gpgpu_num_reg_banks|寄存器组数(default = 8)||-gpgpu_reg_bank_use_warp_id|在将寄存器映射到存储Bank时使用warp id(default = off)||-gpgpu_cache:dl2_texture_only|二级缓存仅用于纹理(0=no, 1=yes, default=1)||操作数收集器配置|  || 选项 | 描述 ||-gpgpu_operand_collector_num_units_sp|采集器单元数（默认值=4）||-gpgpu_operand_collector_num_units_sfu|采集器单元数（默认值=4）||-gpgpu_operand_collector_num_units_mem|采集器单元数（默认值=2）||-gpgpu_operand_collector_num_units_gen|采集器单元数（默认值=0）||-gpgpu_operand_collector_num_in_ports_sp|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_in_ports_sfu|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_in_ports_mem|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_in_ports_gen|端口中采集器单元的数量（默认值=0）||-gpgpu_operand_collector_num_out_ports_sp|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_out_ports_sfu|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_out_ports_mem|端口中采集器单元的数量（默认值=1）||-gpgpu_operand_collector_num_out_ports_gen|端口中采集器单元的数量（默认值=0）||DRAM/Memory控制器配置|  || 选项 | 描述 ||-gpgpu_dram_scheduler &lt;0 = fifo, 1 = fr-fcfs&gt;|DRAM调度程序类型||-gpgpu_frfcfs_dram_sched_queue_size &lt;# entries&gt;|DRAM FRFCFS调度程序队列大小(0 = unlimited (default); # entries per chip)(FIFO调度程序队列大小固定为2)||-gpgpu_dram_return_queue_size &lt;# entries&gt;|DRAM请求返回队列大小 (0 = unlimited (default); # entries per chip)||-gpgpu_dram_buswidth &lt;# bytes/DRAM bus cycle, default=4 bytes, i.e. 8 bytes/command clock cycle&gt;|单个DRAM芯片在命令总线频率下的总线带宽（默认值为4字节（每个命令时钟周期8字节））。每个内存控制器的DRAM芯片数量由选项-gpgpu_n_mem_per_ctrlr设置。每个存储器分区具有（gpgpu_dram_buswidth x gpgpu_n_mem_per_ctrlr）位的DRAM数据总线引脚。例如，Quadro FX5800有一条512位DRAM数据总线，分为8个内存分区。每个存储器分区一个512/8＝64位的DRAM数据总线。该64位总线被分割为每个存储器分区的2个DRAM芯片。每个芯片将具有32位=4字节的DRAM总线宽度。因此，我们将-gpgpu_dram_buswidth设置为4。||-gpgpu_dram_burst_length &lt;# burst per DRAM request&gt;|每个DRAM请求的Burst长度（默认值=4个数据时钟周期，在GDDR3中以2倍命令时钟频率运行）||-gpgpu_dram_timing_opt &lt;nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tWTR&gt;|DRAM时序参数: [nbk = number of banks][tCCD = CAS to CAS command delay (always = half of burst length)][tRRD = Row active to row active delay][tRCD = RAW to CAS delay][tRAS = Row active time][tRP = Row precharge time][tRC = Row cycle time][CL = CAS latency][WL = Write latency][tWTR = Write to read delay]|-gpgpu_mem_address_mask &lt;address decoding scheme&gt;|Obsolete: 选择不同的地址解码方案以在不同的存储体之间扩展内存访问。（0=旧寻址掩码，1=新寻址掩码，2=新添加掩码+翻转的存储体和芯片选择位）|-gpgpu_mem_addr_mapping dramid@&lt;start bit&gt;;&lt;memory address map&gt;|将内存地址映射到DRAM模型：[start bit] = 用于指定DRAM ChannelID的位的起始位置(这意味着下一个Log2(#DRAM channel)位将被用作DRAM ChannelID，并且整个地址映射将根据使用的位数进行移位)。 [memory address map] = 一个64字符的字符串，指定如何将内存地址中的每个位解码为行（R）、列（C）和Bank（B）地址。位于单个DRAM Burst内的部分地址应使用（S）指定。请参见Quadro FX 5800的配置文件。||-gpgpu_n_mem_per_ctrlr &lt;# DRAM chips/memory controller&gt;|每个内存控制器的DRAM芯片数量（也称为DRAM Channel）(aka DRAM channel)||-gpgpu_dram_partition_queues|     i2$:$2d:d2$:$2i||-rop_latency &lt;# minimum cycle before L2 cache access&gt;|指定内存请求到达内存分区时与访问二级缓存/进入队列以访问DRAM之间的最小延迟（以核心时钟周期为单位）。它模拟了最小的L2命中延迟。||-dram_latency &lt;# minimum cycle after L2 cache access and before DRAM access&gt;|指定内存请求访问二级缓存和其被推入DRAM调度器之间的最小延迟（以核心时钟周期为单位）。此选项与-rop_latency一起工作，以模拟最小DRAM访问延迟（=rop_latency+DRAM_latency）。||互连配置|  || 选项 | 描述 ||-inter_config_file &lt;Path to Interconnection Config file&gt;|包含互连网络模拟器选项的文件。有关互连配置的更多详细信息，请参见[6]中随原始代码提供的手册。注意，模拟器中不应使用4.6 Traffic和4.7 Simulation parameters下的选项。另请参阅#互连配置。||-network_mode|要使用的互连网络模式（默认值=1）||PTX配置|  || 选项 | 描述 ||-gpgpu_ptx_use_cuobjdump|使用cuobjdump提取ptx/sass（0=否，1=是）仅限CUDA 4.0||-gpgpu_ptx_convert_to_ptxplus|将嵌入式ptx转换为ptxplus（0=否，1=是）||-gpgpu_ptx_save_converted_ptxplus|将转换后的ptxplus保存到文件（0=否，1=是）||-gpgpu_ptx_force_max_capability|强制最大计算能力（默认值为0）||-gpgpu_ptx_inst_debug_to_file|将执行的指令的调试信息转储到文件中（0=否，1=是）||-gpgpu_ptx_inst_debug_file|输出文件以转储已执行指令的调试信息||-gpgpu_ptx_inst_debug_thread_uid|已执行指令的调试输出的线程uid|\n\n（一）互连配置GPGPU Sim3.x使用booksim路由器模拟器来模拟互连网络。大多数情况下，您需要查阅booksim文档，了解如何配置互连。然而，下面我们列出了需要考虑的特殊注意事项，以确保您的修改与GPGPU-Sim一起使用。\n\n1. 拓扑配置注意，互连网络配置文件中指定的网络节点总数应与GPGPU-Sim中的节点总数相匹配。GPGPU-Sim的节点总数将是SIMT Core集群计数和内存控制器数量的总和。在QuadroFX5800配置中，有10个SIMT Core集群和8个内存控制器。总共有18个节点。因此，在互连配置文件中，网络还具有18个节点，如下所示：\ntopology = fly;k = 18;n = 1;routing_function = dest_tag;\n上面的配置片段建立了一个单级蝶形网络，具有目标标记路由和18个节点。通常，在蝶形网络和网状网络中，网络节点的总数都是k*n。\n请注意，如果选择使用网状网络，则需要考虑配置内存控制器的位置。在当前版本中，可以通过设置use_map=1；来启用一些预定义的映射特别是，我们的ISPASS 2009论文中使用的网状网络可以使用此设置和以下拓扑进行配置：\n\n6x6网格网络（topology=mesh, k=6, n=2）：假设SIMT Core集群大小为1，则为28个SIMT Core+8个DRAM信道。\n\n您可以通过修改interconnect_interface.cpp中的create_node_map()来创建自己的映射（并设置use_map=1）。\n2. GPGPU-Sim添加的Booksim选项这些选项特定于GPGPU Sim，而不是原始booksim的一部分：\n\nperfect_icnt：如果设置了互连不模拟所有注入网络的数据包，那么一个周期后，这些数据包将出现在其目的地。即使当多个源向一个目的地发送数据包时也是如此。\nfixed_lat_per_hop：类似于上面的perfect_icnt，不同的是数据包在“曼哈顿距离跳跃计数乘以fixed_late_per_hop”周期后出现在目的地。\nuse_map：更改内存和shader core的放置方式。请参阅#拓扑配置。\nflit_size：以字节为单位指定flit_size。这用于根据传递给icnt_push()函数的数据包大小来确定每个数据包的微片数。\nnetwork_count：独立互连网络的数量。除非您知道自己在做什么的情况外，应设置为2。\noutput_extra_latency：为每个路由器添加额外的周期。用于创建ISPASS论文的图10。\nenable_link_stats：打印每个链接的额外统计信息。\ninput_buf_size：以微片为单位，每个节点的输入缓冲区大小。如果为零，模拟器将自动设置。参见“从外部世界向网络注入数据包”。\nejection_buffer_size：弹出缓冲区的大小。如果为零，模拟器将自动设置。请参阅“从网络向外部世界弹出数据包”。\nboundary_buffer_size：边界缓冲区大小。如果为零，模拟器将自动设置。请参阅“从网络向外部世界弹出数据包”。\n\n这四个选项是在最初的booksim中使用#define设置的，但我们已经通过intersim的配置文件对它们进行了配置：\nMATLAP_OUTPUT（生成Matlab友好输出）、DISPLAY_LAT_DIST（显示数据包延迟的分布）、DISLAY_HOP_DIST（显示跳数的分布）和DISPLAY_PAR_LATENCY（显示每个源-目标对的平均延迟）。\n3. GPGPU-Sim忽略的Booksim选项请注意，以下选项是原始booksim的一部分，它们要么被忽略，要么不应更改为默认值。\n\n流量选项（booksim手册第4.6节）：injection_rate, injection_process, burst_alpha, burst_beta,const_flit_per_packet, traffic；\n模拟参数（booksim手册第4.7节）：sim_type, sample_period, warmup_periods, max_samples, latency_thres, sim_count, reorder 。\n\n\n（二）时钟域配置GPGPU-Sim支持四个可由-ggpu_clock_domains选项控制的时钟域：\n\nDRAM时钟域=实际DRAM时钟（命令时钟）的频率，而不是数据时钟（即命令时钟频率的2倍）\nSIMT Core集群时钟域=核心时钟中流水线级的频率（即调用SIMT_Core_Cluster::Core_cycle()的速率）\nIcnt时钟域=互连网络的频率（通常这可以被视为NVIDIA GPU规范中的核心时钟）\nL2时钟域=L2缓存的频率（我们通常将其设置为等于Icnt时钟频率）\n\n注意，在GPGPU-Sim中，pipeline的宽度等于warp大小。为了补偿这一点，我们调整了SIMT Core集群时钟域。例如，我们对NVIDIA Quadro FX 5800（GT200）SM中的超流水线级进行了建模，该级以快速时钟速率（1GHz+）运行，单个较慢的流水线级以1/4的频率运行。因此，FX 5800的1.3GHz的shader时钟速率对应于GPGPU Sim中的325MHz SIMT Core时钟。\nDRAM时钟域在命令时钟的频率中指定。为了简化峰值内存带宽计算，大多数GPU规格报告数据时钟，其运行频率是命令时钟频率的两倍。例如，Quadro FX5800的内存数据时钟为1600MHz，而命令时钟仅在800MHz运行。因此，我们的配置将DRAM时钟域设置为800.0MHz。\n1. 时钟专用寄存器在ptx中，有一个特殊的寄存器%clock，用于读取某个时钟周期计数器。在硬件上，这个寄存器被称为SR1。它是一个时钟周期计数器，可以无声地循环。在Quadro中，此计数器每调度程序时钟递增两次。在费米中，每个调度程序时钟只增加一次。GPGPU-Sim将为每个调度程序时钟（与SIMT Core时钟相同）递增一次的计数器返回一个值。\n在PTXPlus中，英伟达编译器生成访问%clock寄存器的指令，如下所示:\n//SASS accessing clock registerS2R R1, SR1SHL R1, R1, 0x1\n//PTXPlus accessing clock registermov %r1, %clockshl %r1, %r1, 0x1\n\n这基本上将时钟寄存器中的值乘以2。然而，在PTX中，直接访问时钟寄存器。在基于时钟寄存器计算结果时，必须考虑这些条件。\n//PTX accessing clock registermov r1, %clock\n\n\n四、理解GPGPU-Sim的输出在每次CUDA网格启动结束时，GPGPU-Sim将性能统计数据打印到控制台（stdout）。这些性能统计数据提供了CUDA应用程序如何使用模拟GPU架构执行的见解。\n以下是重要性能统计数据的简要列表：\n\n（一）一般模拟统计\n\n\n统计\n描述\n\n\n\ngpu_sim_cycle\n执行此内核所需的周期数（以核心时钟为单位）\n\n\ngpu_sim_insn\n在此内核中执行的指令数\n\n\ngpu_ipc\n= gpu_sim_insn / gpu_sim_cycle\n\n\ngpu_tot_sim_cycle\n迄今为止为所有启动的内核模拟的总周期数（以核心时钟为单位）\n\n\ngpu_tot_sim_insn\n迄今为止启动的所有内核执行的指令总数\n\n\ngpu_tot_ipc\n= gpu_tot_sim_insn / gpu_tot_sim_cycle\n\n\ngpu_total_sim_rate\n= gpu_tot_sim_insn / wall_time\n\n\n（二）简单瓶颈分析这些性能计数器跟踪GPU不同高级部分的暂停事件。结合起来，它们给出了应用程序的GPU瓶颈在哪里的广义概念。下图显示出了通过GPGPU-Sim中的存储器子系统的存储器请求的简化流程：\n\n以下是每个计数器的说明：| 统计 | 描述 ||:—-|:—-||gpgpu_n_stall_shd_mem| 由以下原因之一导致的内存阶段的流水线暂停周期数： 共享内存Bank冲突；非合并存储器存取；串行化常数存储器存取。||gpu_stall_dramfull| 互连输出到DRAM Channel的暂停周期数。||gpu_stall_icnt2sh| 由于互连拥塞导致DRAM Channel停滞的周期数。|\n（三）内存访问统计信息\n\n\n统计\n描述\n\n\n\ngpgpu_n_load_insn\n执行的全局/本地加载指令数\n\n\ngpgpu_n_store_insn\n执行的全局/本地存储指令数\n\n\ngpgpu_n_shmem_insn\n执行的共享内存指令数\n\n\ngpgpu_n_tex_insn\n执行的纹理内存指令数\n\n\ngpgpu_n_const_mem_insn\n执行的常量内存指令数\n\n\ngpgpu_n_param_mem_insn\n执行的参数读取指令数\n\n\ngpgpu_n_cmem_portconflict\n常量内存组冲突的数量\n\n\nmaxmrqlatency\n最大内存队列延迟（内存请求在DRAM内存队列中花费的时间）\n\n\nmaxmflatency\n最大内存获取延迟（从Sahder Core到DRAM和返回的往返延迟）\n\n\naveragemflatency\n平均内存提取数据延迟\n\n\nmax_icnt2mem_latency\n内存请求从Sahder Core穿越到目标DRAM Channel的最大延迟\n\n\nmax_icnt2sh_latency\n内存请求从DRAM Channel返回指定Sahder Core的最大延迟\n\n\n（四）控制流统计信息GPGPU-Sim报告了warp占用率分布，该分布测量了CUDA应用程序中分支分歧导致的性能损失。该信息在文本&quot;Warp Occupancy Distribution:&quot;后的单行中报告。或者，您可以grep W0_Idle。分布以以下格式显示：＜bin＞:＜cycle count＞。以下是每个bin的含义：| 统计 | 描述 ||:—-|:—-||Stall|Sahder Core流水线暂停且无法发出任何指令时的周期数||W0_Idle|当所有可用的warp都被发送到流水线并且没有准备好执行下一条指令时的周期数||W0_Scoreboard|所有可用warp都在等待内存中的数据时的周期数||WX (where X = 1 to 32)|将具有X个活动线程的warp调度到流水线中时的周期数|\n没有分支发散的代码应该不会产生WX的循环，其中X介于1和31之间。有关详细信息，请参阅动态warp形成：SIMD图形硬件上的高效MIMD控制流。\n（五）DRAM统计信息默认情况下，GPGPU-Sim报告每个DRAM Channel的以下统计信息：| 统计 | 描述 ||:—-|:—-||n_cmd| DRAM Channel中内存控制器已消逝的命令周期总数。控制器可以在每个命令周期发出单个命令。||n_nop| 内存控制器发出的NOP命令总数。||n_act| 内存控制器发出的Row Activation命令总数。||n_pre| 内存控制器发出的Precharge命令总数。||n_req| DRAM Channel处理的内存请求总数。||n_rd| 内存控制器发出的读取命令总数。||n_write| 内存控制器发出的写入命令总数。||bw_util| DRAM带宽利用率=2*(n_rd+n_write)/n_cmd。||n_activity| 内存控制器队列中有挂起请求时的活动周期或命令周期总数。||dram_eff| DRAM效率=2*(n_rd+n_write)/n_activity（即，当有待处理的挂起请求时的DRAM带宽利用率）。||mrqq: max| 最大内存请求队列占用率。（即队列中挂起条目的最大数量）。||mrqq: avg| 平均内存请求队列占用率。（即队列中挂起条目的平均数量）。|\n（六）Cache统计信息对于每个缓存（普通数据缓存、常量缓存、纹理缓存等），GPGPU Sim报告以下统计信息：\n\nAccess=访问缓存的总次数\nMiss=缓存未命中的总数。括号中的数字是缓存未命中率。\nPendingHit=缓存中挂起的命中总数。挂起的命中访问已命中处于RESERVED状态的缓存行，这意味着同一行上已存在由先前缓存未命中发送的飞行内存请求。此访问可以与先前的内存访问合并，从而不会产生内存流量。括号中的数字是显示挂起命中的访问的比率。\n\n请注意，挂起的命中不算作缓存未命中。此外，对于采用填充时分配策略的缓存（例如，只读缓存，如常量缓存和纹理缓存），我们不计算挂起命中。\nGPGPU-Sim还计算L1数据缓存的所有实例的总未命中率：\n\ntotal_dl1_misses\ntotal_dl1_accesses\ntotal_dl1_miss_rate\n\n请注意，当L1缓存关闭时，应忽略L1 Total Miss Rate的数据：-gpgpu_cache:dl1 none\n（七）互连统计信息在GPGPU-Sim中，用户可以配置是在单个互连网络上运行所有流量，还是在两个单独的物理网络上运行（一个将数据从Sahder Core中继到DRAM Cahnnel，另一个则将数据中继回来）。（除了增加带宽外，使用两个独立网络的动机通常是为了避免协议死锁（protocol deadlock），否则需要额外的专用虚拟通道。）GPGPU-Sim报告了每个独立互连网络的以下统计数据：| 统计 | 描述 ||:—-|:—-||average latency|单个微片从源节点到目标节点的平均延迟。||average accepted rate|相对于总输入channel吞吐量，测量的网络平均吞吐量。请注意，当使用两个单独的网络进行不同方向的流量时，某些节点将永远不会将数据注入网络（即，仅输出的节点，如核心上的DRAM Channel）。要获得实际比率，应忽略这些节点的总输入channel吞吐量。这意味着应该将该速率与比率（该网络中的total #nodes/# input nodes）相乘，以获得实际平均接受速率。注意，默认情况下，我们使用两个独立的网络，这是由互连网络配置文件中的network_count选项设置的。这两个网络用于打破可能导致死锁的循环依赖关系。||min accepted rate|始终为0，因为有些节点不会向网络中注入微片，这是因为我们模拟了两个不同方向的流量网络。||latency_stat_0_freq|显示网络中经过的微片的延迟分布的直方图。|\n注：网络的接受流量或吞吐量是指传送到网络目标终端的流量。如果网络低于饱和，则网络接受所有提供的流量，并且提供的流量将等于网络的吞吐量。互连模拟器通过将在节点处接收的分组总数除以总网络周期来计算每个节点的接受率。\n\n五、可视化高层次GPGPU Sim微体系结构行为AerialVision是GPGPU-Sim的GPU性能分析工具，包含在ISPASS 2010论文中介绍的GPGPU-Sim源代码中。可以在此处找到详细的手册，介绍如何使用AerialVision。\nAerialVision可能进行的高级分析类型的两个示例如下所示。下面第一张图说明了使用AerialVision来理解微体系结构行为与时间的关系。顶行是平均内存访问延迟与时间的关系，第二行是每个SIMT Core的负载与时间的图（纵轴是SIMT Core，颜色表示每个周期的平均指令），底行显示每个内存控制器通道的负载。下面第二张图说明了使用AerialVision在源代码级别理解微体系结构行为。这有助于识别与未合并或分支分歧相关的代码行。\n\n\n要让GPGPU-Sim为Time-Lapse View生成可视化工具日志文件，请在gpgpusim.config中添加以下选项：\n-visualizer_enabled 1\n\n此日志文件中的采样频率可以通过选项-ggpu_runtime_stat设置。还可以使用-videalizer_outputfile选项指定可视化工具日志文件的名称。\n要生成源代码查看器的输出，请向gpgpusim.config添加以下选项：\n-enable_ptx_file_line_stats 1\n\n可以使用选项-ptx_line_stats_filename指定输出文件名。\n如果您在出版物中使用AerialVision生成的绘图，请引用上面链接的ISPASS 2010论文。\n\n六、可视化逐周期微体系结构行为GPGPU Sim 3.x提供了一组GDB宏，可用于可视化每个SIMT Core和内存分区的详细状态。通过在GDB中设置全局变量g_single_step到你想开始single stepping Shader时钟的Shader时钟周期，这些宏可以用来观察微架构状态如何逐周期变化。您可以在GDB停止模拟器的任何时候使用宏，但gpu-sim.cc中使用全局变量g_single_step来触发对硬编码软件断点指令的调用，该指令在所有Shader Core都已完成一个周期的高级模拟后放置。在这里停止模拟会导致更容易解释状态。\n此级别的可见性对于调试非常有用，可以帮助您深入了解模拟GPU微架构的工作原理。\nGPGPU-Sim发行版附带的.gdbinit文件中提供了这些GDB宏。要使用这些宏，请将文件复制到主目录或启动GDB的同一目录。GDB将自动检测宏文件的存在，加载宏文件并显示以下消息：\n** loading GPGPU-Sim debugging macros... **\n\n\n\n宏\n描述\n\n\n\ndp &lt;core_id&gt;\n显示ID=&lt;core_id&gt;的SIMT Core的流水线状态。显示的例子见下文。\n\n\ndpc &lt;core_id&gt;\n显示流水线状态，然后继续到下一个断点。如果你设置了g_single_step来触发硬编码的断点，其中gpu_sim_cycle在 src/gpgpu-sim/gpu-sim.cc中被递增，那么这个版本就很有用。反复点击回车键将推进显示连续循环中的流水线内容。\n\n\ndm &lt;partition_id&gt;\n显示一个ID=&lt;partition_id&gt;的内存分区的内部状态。\n\n\nptxdis &lt;start_pc&gt; &lt;end_pc&gt;\n显示&lt;start_pc&gt;和&lt;end_pc&gt;之间的PTX指令。\n\n\nptxdis_func &lt;core_id&gt; &lt;thread_id&gt;\n显示SIMT Core&lt;core_id&gt;中由线程&lt;thread_id&gt;执行的内核函数内的所有PTX指令。\n\n\nptx_tids2pcs &lt;thread_ids&gt; &lt;length&gt; &lt;core_id&gt;\n显示SIMT Core&lt;core_id&gt;中的线程的当前PC，由一个长度=&lt;length&gt;的数组&lt;thread_ids&gt;表示。\n\n\ndp输出示例：\n\n\n七、性能模拟中的调试错误本节介绍在性能模拟模式下运行时调试GPGPU-Sim崩溃或死锁的错误的策略。\n如果你运行了一个我们没有测试过的基准测试，它崩溃了，我们鼓励你提交一个bug。\n然而，假设GPGPU-Sim在您进行了自己的更改后崩溃或死锁？那你该怎么办？本节介绍我们在UBC用于调试GPGPU Sim时序模型中的错误的一些通用技术。\n\nSegmentation Faults, Aborts and Failed Assertions：本节无内容\nDeadlocks：本节无内容\n\n\n八、常问问题问题：如何计算GPGPU-Sim配置下的片外DRAM峰值带宽？\n回答：片外DRAM的峰值带宽=gpgpu_n_mem * gpgpu_n_mem_per_ctrlr * gpgpu_dram_buswidth * DRAM Clock * 2 (for DDR)。其中：\n\ngpgpu_n_mem = GPU中的内存通道数（每个内存通道都有一个独立的控制器用于DRAM命令调度）\ngpgpu_n_mem_per_ctrlr = 连接到一个内存通道的DRAM芯片的数量（默认=2，用于64位内存通道）\ngpgpu_dram_buswidth = 每个DRAM芯片的总线宽度（默认=32位=4字节）\nDRAM Clock = DRAM芯片的真实时钟（相对于市场中使用的有效时钟–参见#时钟域配置）。\n\n第四部分：GPGPU-Sim的软件设计要使用GPGPU Sim 3.x进行实质性的架构研究，您需要修改源代码。本节介绍GPGPU Sim 3.x的高级软件设计，它与2.x版不同。除了此处的软件描述外，您可能会发现参考GPGPU Sim3.x的doxygen生成文档会有所帮助。请参阅README文件以获取从GPGPU Sim3.x源代码构建doxygen文档的说明。\n下面我们总结了源码组织、命令行选项解析器、面向对象的抽象硬件模型，该模型提供了性能模拟引擎的软件组织和功能模拟引擎的软件组织之间的接口。最后，我们描述了CUDA/OpenCL应用程序接口的软件设计。\n\n一、文件列表和简要说明GPGPU Sim 3.x由三个主要模块组成（每个模块位于自己的目录中）：\n\ncuda-sim：执行N虚拟ChannelC或OpenCL编译器生成的PTX内核的功能模拟器。\ngpgpu-sim：模拟GPU（或其他许多核心加速器架构）计时行为的性能模拟器。\nintersim：采用Bill Dally的BookSim的互连网络模拟器。以下是每个模块中的文件：\n\n（一）Overall/Utilities\n\n\n文件名\n描述\n\n\n\nMakefile\n构建gpgpu-sim的Makefile，并调用cuda-sim和intersim中的其他Makefile。\n\n\nabstract_hardware_model.h/.cc\n提供一套功能模拟器和时序模拟器之间的接口类。\n\n\ndebug.h/.cc\n实现了交互式调试器模式。\n\n\ngpgpusim_entrypoint.c\n包含与CUDA/OpenCL API库的接口函数。\n\n\noption_parser.h/.cc\n实现了命令行选项解析器。\n\n\nstream_manager.h/.cc\n实现流管理器以支持CUDA流。\n\n\ntr1_hash_map.h\n包含C++11中std::unordered_map的封装代码，如果编译时不支持C++11，则返回到std::map或GNU hash_map。\n\n\n.gdb_init\n包含的宏可以简化GDB仿真状态的底层可视化。\n\n\n（二）cuda-sim\n\n\n文件名\n描述\n\n\n\nMakefile\ncuda-sim的Makefile。由上一级的Makefile调用。\n\n\ncuda_device_print.h/.cc\n实现支持CUDA设备函数中的printf()（即在GPU内核函数中调用printf()）。请注意，设备printf仅适用于CUDA 3.1。\n\n\ncuda-math.h\n包含CUDA Math头文件的接口。\n\n\ncuda-sim.h/.cc\n实现了gpgpu-sim和cuda-sim之间的接口。它还包含一个独立的功能仿真器。\n\n\ninstructions.h/.cc\n所有PTX指令的仿真代码都在这里实现。\n\n\nmemory.h/.cc\n功能性内存空间仿真。\n\n\nopcodes.def\nDEF文件在每条指令的各种信息之间建立联系（例如，字符串名称、实现、内部操作码……）。\n\n\nopcodes.h\n为每个PTX指令定义枚举。\n\n\nptxinfo.l/.y\nLex和Yacc文件用于解析ptxinfo文件。(为了获得内核资源需求)\n\n\nptx_ir.h/.cc\nCUDA中的静态结构–内核、函数、符号…等。也包含了执行静态分析的代码，用于在加载时从内核中提取 immediate-post-dominators。\n\n\nptx.l/.y\nLex和Yacc文件用于解析.ptx文件和嵌入式cubin结构，以获得CUDA内核的PTX代码。\n\n\nptx_loader.h/.cc\n包含用于加载和解析以及打印PTX和PTX信息文件的函数。\n\n\nptx_parser.h/.cc\n包含Yacc在解析过程中调用的函数，用于创建功能和性能模拟所需的infra结构。\n\n\nptx_parser_decode.def\n包含在PTX提取中使用的解析器的标记定义。\n\n\nptx_sim.h/.cc\nCUDA的动态结构–网格、CTA、线程。\n\n\nptx-stats.h/.cc\nPTX的source line分析器。\n\n\n（三）gpgpu-sim\n\n\n文件名\n描述\n\n\n\nMakefile\ngpgpu-sim的Makefile。由上一级的Makefile调用。\n\n\naddrdec.h/.cc\n地址解码器 - 将一个给定的地址映射到DRAM Channel中的一个特定的行、Bank、列。\n\n\ndelayqueue.h\n一个灵活的流水线队列的实现。\n\n\ndram_sched.h/.cc\nFR-FCFS DRAM请求调度器。\n\n\ndram.h/.cc\nDRAM时序模型 + gpgpu-sim其他部分的接口。\n\n\ngpu-cache.h/.cc\nGPGPU-Sim的缓存模型。\n\n\ngpu-misc.h/.cc\n包含gpgpu-sim部分所需的杂项功能。\n\n\ngpu-sim.h/.cc\n将GPGPU-Sim中不同的时序模型粘在一起。它包含了支持多个时钟域的实现，并实现了线程块调度器。\n\n\nhistogram.h/.cc\n定义了几个实现不同种类柱状图的类。\n\n\nicnt_wrapper.h/.c\ngpgpu-sim的互连网络接口。它提供一个完全解耦的接口，允许intersim作为gpgpu-sim的互连网络时序模拟器工作。\n\n\nl2cache.h/.cc\n实现了一个内存分区的时序模型。它还实现了二级缓存，并与内存分区的其他部分（例如DRAM时序模型）进行了接口。\n\n\nmem_fetch.h/.cc\n定义了mem_fetch，一个模拟内存请求的通信结构。\n\n\nmem_fetch_status.tup\n定义了内存请求的状态。\n\n\nmem_latency_stat.h/.cc\n包含用于收集内存系统统计信息的各种代码。\n\n\nscoreboard.h/.cc\n实现了SIMT Core中使用的记分牌。\n\n\nshader.h/.cc\nSIMT Core的时序模型。它调用cudu-sim对一个特定的线程进行功能模拟，cuda-sim将返回该线程的性能敏感的信息。\n\n\nstack.h/.cc\nimmediate-post-dominator线程调度器使用的简单堆栈。(已废止)\n\n\nstats.h\n定义了对内存访问和内存流水线的各种滞留条件进行分类的枚举。\n\n\nstat-tool.h/.cc\n实现各种性能测量的工具。\n\n\nvisualizer.h/.cc\n为可视化器输出动态统计数据。\n\n\n（四）intersim仅列出从原始Booksim修改的文件：| 文件名 | 描述 ||:—-|:—-||booksim_config.cpp|intersim的配置选项在此定义，并给出了一个默认值。||flit.hpp|修改后，增加了为flits携带数据的能力。flits也知道它们属于哪个网络。||interconnect_interface.cpp/.h|GPGPU-Sim和intersim之间的接口在此实现。||iq_router.cpp/.hpp|修改后添加了对output_extra_latency的支持（用于创建ISPASS 2009论文的图10）。|islip.cpp|进行了一些小的编辑，以修复一个数组边界外的错误。||Makefile|修改后，创建了一个库，而不是独立的网络模拟器。||stats.cpp/.hpp|统计收集功能在这个文件中。我们做了一些小调整。例如，我们添加了一个名为NeverUsed的新函数，它可以告诉我们这个特定的统计数据是否被更新过。||statwraper.cpp/.h|一个封装器，可以在C文件中使用stats.cpp中的Stat类实现的状态收集功能。||trafficmanager.cpp/.hpp|对原始的booksim进行了大量的修改。许多高水平的操作在这里完成。|\n\n二、选项解析器当你为你的研究修改GPGPU-Sim时, 你可能会增加一些你想在不同的模拟中进行不同配置的功能。GPGPU-Sim 3.x提供了一个通用的命令行选项解析器，允许不同的软件模块通过一个简单的接口来注册他们的选项。选项解析器在gpgpusim_entrypoint.cc的gpgpu_ptx_sim_init_perf() 中实例化。选项在gpgpu_sim_config::reg_options()中使用函数添加。\nvoid option_parser_register(option_parser_t opp,                             const char *name,                             enum option_dtype type,                             void *variable,                             const char *desc,                              const char *defaultvalue);\n下面是每个参数的描述:\n\noption_parser_t opp：选项解析器的标识。\nconst char *name：识别命令行选项的字符串。\nenum option_dtype type：选项的数据类型，它可以是以下之一：\nint\nunsigned int\nlong long\nunsigned long long\nbool(如C语言中的int)\nfloat\ndouble\nc-string (即char*)\n\n\nvoid *variable：指向变量的指针。\nconst char *desc：显示选项的描述。\nconst char *defaultvalue：选项的默认值（字符串值将被自动解析）。对于这个c-string变量，你可以将其设置为NULL。\n\n可以查看gpgpu-sim/gpu-sim.cc里面的例子。\n选项解析器是通过option_parser.cc中的OptionParser类实现的（在C接口中作为option_parser_t暴露在option_parser.h中）。这里是GPGPU-Sim其他部分使用的完整C接口：\n\noption_parser_register()：在一个选项名称（一个字符串）和模拟器中的一个变量之间创建一个绑定。该变量是通过引用（指针）传递的，当调用option_parser_cmdline()或option_parser_cfgfile()时它将被修改。注意，每个选项只能绑定到一个变量。\noption_parser_cmdline()：解析给定的命令行选项。为选项-config &lt;config filename&gt;调用option_parser_cmdline()。\noption_parser_cfgfile()：解析一个包含配置选项的给定文件。\noption_parser_print()：打印所有注册的选项和它们的解析值。\n\n在GPGPU-Sim中只有一个OptionParser对象被实例化，在gpgpusim_entrypoint.cc的gpgpu_ptx_sim_init_perf()中。这个OptionParser对象将gpgpusim.config中的模拟选项转换为可以在模拟器中访问的变量值。GPGPU-Sim中的不同模块将它们的选项注册到OptionParser对象中（即指定哪个选项对应于模拟器中的哪个变量）。之后，模拟器调用Option_parser_cmdline()来解析gpgpusim.config中包含的仿真选项。\n在内部，OptionParser类包含一组OptionRegistry。OptionRegistry是一个模板类，使用&gt;&gt;操作符进行解析。OptionRegistry的每个实例负责将一个选项解析为一个特定类型的变量。目前该解析器只支持以下数据类型，但可以通过重载&gt;&gt;操作符来扩展对更复杂数据类型的支持：\n\n32-bit/64-bit integers\nfloating points (float and doubles)\nbooleans\nc-style strings (char*) \n\n\n三、抽象硬件模型文件abstract_hardware_model.h/.cc提供了一套功能模拟器和时序模拟器之间的接口类。\n（一）硬件抽象模型对象\n\n\n枚举名称\n描述\n\n\n\n_memory_space_t\n内存空间类型（register, local, shared, global, …）。\n\n\nuarch_op_t\n操作类型（ALU_OP, SFU_OP, LOAD_OP, STORE_OP, …）。\n\n\n_memory_op_t\n定义指令是否访问内存（load or store）。\n\n\ncudaTextureAddressMode\nCUDA纹理地址模式。它可以指定包装地址模式，钳制到边缘地址模式，镜像地址模式或边界地址模式（wrapping address mode, clamp to edge address mode, mirror address mode or border address mode）。\n\n\ncudaTextureFilterMode\nCUDA纹理过滤模式（point or linear filter mode）。\n\n\ncudaTextureReadMode\nCUDA纹理读取模式。指定读取纹理模式为元素类型或归一化浮点（element type or normalized float）。\n\n\ncudaChannelFormatKind\nCUDA运行时使用的数据类型，指定通道格式。这是cudaCreateChannelDesc(...)的一个参数。\n\n\nmem_access_type\n在时序模拟器中对不同类型的存储器进行不同的访问。\n\n\ncache_operator_type\nPTX提供的不同类型的L1数据缓存访问行为。\n\n\ndivergence_support_t\n不同的控制流发散处理模型（支持post dominator）。\n\n\n类名\n描述\n\n\nclass kernel_info_t\n保存一个内核的信息。它包含的信息有：内核的功能（function_info），网格（Grid）和块（Block size）的大小，以及该内核内活跃线程的列表（ptx_thread_info）。\n\n\nclass core_t\n内核的抽象基类，用于功能和性能模型。shader_core_ctx（在时序模型中实现SIMT Core的类）来源于这个类。\n\n\nstruct cudaChannelFormatDesc\n通道描述符结构。它保持通道格式和每个组件的位数。\n\n\nstruct cudaArray\n用于保存GPGPU-Sim中数组数据的结构。它在主程序调用cuda_malloc、cuda_memcopy、cuda_free等时使用。\n\n\nstruct textureReference\ncuda运行时使用的数据类型，用于指定纹理参考。\n\n\nclass gpgpu_functional_sim_config\n功能仿真器配置选项。\n\n\nclass gpgpu_t\n实现GPU功能模拟器的顶层类。它包含功能模拟器的配置（gpgpu_functional_sim_config），并持有实现全局/纹理内存空间的实际缓冲器（memory_space类的实例）。它有一组成员函数，提供对模拟的GPU内存空间的管理（malloc, memcpy, texture-bindin, …）。这些成员函数被CUDA/OpenCL的API实现所调用。gpgpu_sim类（顶级的GPU时序仿真模型）是由该类派生的。\n\n\nstruct gpgpu_ptx_sim_kernel_info\n保存内核函数的属性，如PTX版本和目标机。也保存该内核所使用的内存和寄存器的数量。\n\n\nstruct gpgpu_ptx_sim_arg\n保存内核参数/参数的信息，分别在CUDA和OpenCL的cudaSetupArgument(...)和_cl_kernel::bind_args(...)设置。\n\n\nclass memory_space_t\n内存空间的信息，如内存的类型和该内存空间的Bank的数量。\n\n\nclass mem_access_t\n包含时序模拟器中每个内存访问的信息。该类包含内存访问的类型、请求的地址、数据的大小以及访问内存的warp的活动掩码等信息。该类被用作mem_fetch类的参数之一，该类基本上为每个内存访问实例化。这个类是用于两个不同级别的内存之间的接口，并通过互连。\n\n\nstruct dram_callback_t\n这个类是负责原子操作的。函数指针在功能仿真期间（atom_impl()）被设置为atom_callback(...)。在时序模拟中，当L2 Cache内存控制器将内存分区单元弹出到互连上时，这个函数指针被调用。这个函数应该是计算原子操作的结果并将其保存在内存中。\n\n\nclass inst_t\n所有指令类的基类。这个类包含了指令的类型和大小，指令的地址，输入和输出，延迟和指令的内存范围（memory_space_t）等信息。\n\n\nclass warp_inst_t\n时序仿真中需要的指令数据。每条指令（ptx_instruction）都继承自warp_inst_t，包含用于时序和功能仿真的数据。ptx_instruction在功能仿真时被填充。在这一级之后，程序只需要时序信息，所以它将ptx_instruction转为warp_inst_t（一些数据被释放）用于时序模拟。它持有warp_id、warp内的活动线程掩码、内存访问列表（mem_access_t）和该warp内线程的信息（per_thread_info）。\n\n\n\n四、GPGPU-sim - 性能仿真引擎在GPGPU-Sim 3.x中，性能仿真引擎是通过&lt;gpgpu-sim_root&gt;/src/gpgpu-sim/下的文件中定义和实现的许多类来实现的。这些类通过顶层类gpgpu_sim汇集在一起，该类是由gpgpu_t（其功能仿真对应部分）派生的。在当前版本的GPGPU-Sim 3.x中，模拟器中只有一个gpgpu_sim的实例g_the_gpu。目前不支持同时对多个GPU进行仿真，但在未来的版本中可能会提供。\n本节描述了性能仿真引擎中的各种类。这些包括一套模拟#微架构模型部分描述的微架构的软件对象，本节还描述了性能仿真引擎如何与功能仿真引擎对接，如何与AerialVision对接，以及我们在性能仿真引擎中采用的各种非琐碎软件设计。\n（一）性能模型软件对象GPGPU-Sim 3.x与2.x相比，更重要的变化之一是为性能仿真引擎引入了C++（大部分）面向对象的设计。用于实现性能仿真引擎的各种类的高层设计将在本小节中描述。这些与前面描述的#硬件模型部分密切相关。\n1. SIMT Core集群类#SIMT Core集群是由simt_core_cluster类来模拟的。这个类在m_core中包含了一个SIMT Core对象的数组。simt_core_cluster::core_cycle()方法简单地按顺序循环每个SIMT Core。simt_core_cluster::icnt_cycle()方法将内存请求从互连网络推入SIMT Core集群的响应FIFO。它还从FIFO中弹出请求，并将其发送到相应的Core的指令缓存或LDST单元。simt_core_cluster::icnt_inject_request_packet(...)方法为SIMT Core提供了一个向网络注入数据包的接口。\n\n2. SIMT Core类#上面SIMT Core图中所示的#SIMT Core微架构是由shader.h/cc中的shader_core_ctx类实现的。这个类是从core_t类（内核的抽象功能类）派生出来的，它结合了所有不同的对象，实现了SIMT Core微架构模型的各个部分：\n\n一个shd_warp_t对象的集合，它模拟了内核中每个warp的模拟状态。\n一个SIMT堆栈，simt_stack对象，为每个warp处理分支分歧。\n一组scheduler_unit对象，每个对象负责从其warp集中选择一条或多条指令，并发射这些指令进行执行。\n一个用于检测数据危险的Scoreboard对象。\n一个opndcoll_rfu_t对象，用于模拟一个操作数收集器。\n一组simd_function_unit对象，它实现了SP单元和SFU单元（ALU流水线）。\n一个ldst_unit对象，它实现了内存流水线。\n一个shader_memory_interface，将SIMT Core与相应的SIMT Core集群连接起来。每个内存请求都要经过这个接口，由其中一个内存分区提供服务。\n\n每个内核周期，shader_core_ctx::cycle()都被调用，以模拟SIMT Core的一个周期。这个函数调用一组成员函数，按相反的顺序模拟内核的流水线阶段，以模拟流水线效应。\n\nfetch()\ndecode()\nissue()\nread_operand()\nexecute()\nwriteback()\n\n各个流水线阶段通过一组流水线寄存器连接，这些寄存器是指向warp_inst_t对象的指针（除了Fetch和Decode，它们通过ifetch_buffer_t对象连接）。\n每个shader_core_ctx对象在访问SIMT Core特定的配置选项时，都会引用一个通用的shader_core_config对象。所有的shader_core_ctx对象还链接到一个共同的shader_core_stats对象的实例，该对象跟踪所有SIMT Core的性能测量值的集合。\n（1）Fetch和Decode软件模型本节介绍软件建模#Fetch和Decode的情况。\n#Simt-core图所示的I-Buffer被实现为shader_core_ctx内部的shd_warp_t对象阵列。每个shd_warp_t都有一组m_ibuffer的I-Buffer条目(ibuffer_entry)，持有可配置的指令数量（一个周期内允许获取的最大指令）。另外，shd_warp_t有一些标志，这些标志被调度器用来确定warp的发射资格。存储在ibuffer_entry中的解码指令是一个指向warp_inst_t对象的指针。warp_inst_t持有关于这条指令的操作类型和所用操作数的信息。\n另外，在Fetch阶段，shader_core_ctx::m_inst_fetch_buffer变量在获取（指令缓存访问）和解码阶段之间充当流水线寄存器。\n如果解码阶段没有停滞（即shader_core_ctx::m_inst_fetch_buffer没有有效的指令），取数单元就会工作。外部for循环实现了循环调度，最后一个调度的warp ID被存储在m_last_warp_fetched。第一个if语句检查warp是否已经执行完毕，而在第二个if语句中，如果是命中，则从指令缓存中实际获取，如果是未中，则生成内存访问。第二个if语句主要是检查是否有有效的指令已经存储在与当前检查的warp对应的条目中。\nDecode阶段只是检查shader_core_ctx::m_inst_fetch_buffer，并开始将解码后的指令（当前配置每个周期最多解码两条指令）存储在指令缓冲区条目（m_ibuffer，shd_warp_t::ibuffer_entry的一个对象）中，该条目与shader_core_ctx::m_inst_fetch_buffer中的warp对应。\n（2）调度器和发射软件模型在每个Core中，都有可配置数量的调度器单元。函数shader_core_ctx::issue()在这些单元上进行迭代，其中每一个单元都执行scheduler_unit::cycle()，在这里对warp采用轮循算法。在scheduler_unit::cycle()中，指令使用shader_core_ctx::issue_warp()函数被发射到其合适的执行流水线。在这个函数中，指令通过调用shader_core_ctx::func_exec_inst()在功能上被执行，SIMT栈（m_simt_stack[warp_id]）通过调用simt_stack::update()被更新。另外，在这个函数中，由于障碍的存在，通过shd_warp_t:set_membar()和barrier_set_t::warp_reaches_barrier来保持/释放warp。另一方面，寄存器被Scoreboard::reserveRegisters()保留，以便以后被记分牌算法使用。scheduler_unit::m_sp_out, scheduler_unit::m_sfu_out, scheduler_unit::m_mem_out指向SP、SFU和Mem流水线接收的发射阶段和执行阶段之间的第一个流水线寄存器。这就是为什么在使用shader_core_ctx::issue_warp()向其相应的流水线发出任何指令之前要检查它们。\n（3）SIMT堆栈软件模型对于每个调度器单元，有一个SIMT堆栈阵列。每个SIMT堆栈对应一个warp。在scheduler_unit::cycle()中，被调度的warp的SIMT栈的栈顶条目决定了发出的指令。栈顶条目的程序计数器通常与I-Buffer中的下一条指令的程序计数器一致，该指令对应于预定的warp（参考#SIMT堆栈）。否则，在控制冒险的情况下，它们将不被匹配，I-Buffer内的指令被刷新。\nSIMT堆栈的实现是在shader.h中的simt_stack类。SIMT堆栈在每次发射后都会使用这个函数simt_stack::update(..)进行更新。这个函数实现了发散点和再发散点所需的算法。在更新SIMT栈之前，功能执行（参考#指令执行）是在发射阶段进行的。这允许发射阶段拥有每个线程的下一个PC的信息，因此，可以根据需要更新SIMT堆栈。\n（4）计分牌软件模型记分牌单元在shader_core_ctx中被实例化为一个成员对象，并通过引用（指针）传递给scheduler_unit。它同时存储Shader Core的ID和一个由warp ID索引的寄存器表。这个寄存器表存储了每个warp所保留的寄存器的数量。Scoreboard::reserveRegisters(...), Scoreboard::releaseRegisters(...) 和 Scoreboard::checkCollision(...) 函数分别用于保留寄存器、释放寄存器以及在发射warp前检查是否有冲突。\n（5）操作数收集器软件模型操作数收集器被建模为主流水线中的一个阶段，由函数shader_core_ctx::cycle()执行。这个阶段由shader_core_ctx::read_operands()函数表示。关于操作数收集器的接口，请参考#ALU流水线的更多细节。\nopndcoll_rfu_t类是基于操作数收集器的寄存器文件单元的模型。它包含了对收集器单元集、仲裁器和调度单元进行抽象的类。\nopndcoll_rfu_t::allocate_cu(...)负责将warp_inst_t分配给其指定的操作数收集器组中的空闲操作数收集器单元。同时，它在仲裁器的相应Bank队中为所有的源操作数增加一个读取请求。\n然而，opndcoll_rfu_t::allocate_reads(...)处理没有冲突的读请求，换句话说，在不同寄存器Bank中的读请求和不去同一个操作数收集器的读请求会从仲裁器队列中弹出。这说明写请求的优先级高于读请求。\n函数opndcoll_rfu_t::dispatch_ready_cu()将准备好的操作数收集器的操作数寄存器（所有操作数都已收集）分配到执行阶段。\n函数opndcoll_rfu_t::writeback(const warp_inst_t &amp;inst)在内存流水线的写回阶段被调用。它负责写的分配。\n以上总结了用于模拟操作数收集器的主要函数的要点，然而，更多的细节在shader.cc和shader.h中的opndcoll_rfu_t类的实现中。\n（6）ALU流水线软件模型SP单元和SFU单元的时序模型主要在shader.h中定义的pipelined_simd_unit类中实现。模拟单元的具体类（sp_unit类和sfu类）是从这个类派生出来的，有重载的can_issue()成员函数来指定单元可执行的指令类型。\nSP单元通过OC_EX_SP流水线寄存器连接到操作收集器单元；SFU单元通过OC_EX_SFU流水线寄存器连接到操作数收集器单元。两个单元通过WB_EX流水线寄存器共享一个共同的写回阶段。为了防止两个单元因写回阶段的冲突而停滞，每条进入任何一个单元的指令都必须在发出到目标单元之前在结果总线（m_result_bus）上分配一个槽（见shader_core_ctx::execute()）。\n下图提供了一个概览，介绍了pipelined_simd_unit如何为不同类型的指令建立吞吐量和延迟。\n\n在每个pipelined_simd_unit中，issue(warp_inst_t*&amp;)成员函数将给定的流水线寄存器的内容移入m_dispatch_reg。然后指令在m_dispatch_reg等待initiation_interval个周期。在此期间，没有其他的指令可以发到这个单元，所以这个等待是指令的吞吐量的模型。等待之后，指令被派发到内部流水线寄存器m_pipeline_reg进行延迟建模。派遣的位置是确定的，所以在m_dispatch_reg中花费的时间也被计入延迟中。每个周期，指令将通过流水线寄存器前进，最终进入m_result_port，这是共享的流水线寄存器，通向SP和SFU单元的共同写回阶段。\n各类指令的吞吐量和延迟在cuda-sim.cc的ptx_instruction::set_opcode_and_latency()中指定。这个函数在预解码时被调用。\n（7）内存阶段的软件模型shader.cc中的ldst_unit类实现了Shader流水线的内存阶段。该类实例化并操作所有的Shader内存：纹理（m_L1T）、常量（m_L1C）和数据（m_L1D）。ldst_unit::cycle()实现了该单元操作的核心，并在核心周期前被泵入m_config-&gt;mem_warp_parts次数。ldst_unit::cycle()处理来自互连的内存响应（存储在m_response_fifo中），填充缓存并标记存储为完成。该函数还循环缓存，以便它们可以向互连发送它们对错过的数据的请求。\n对每种类型的L1内存的缓存访问分别在shared_cycle()、constant_cycle()、texture_cycle()和memory_cycle()中完成。 memory_cycle用于访问L1数据的缓存。这些函数中的每一个都会调用process_memory_access_queue()，这是一个通用函数，从指令的内部访问队列中抽取一个访问，并将这个请求发送到缓存中。如果这个访问在这个周期内不能被处理（也就是说，它既没有错过也没有命中缓存，这可能发生在各种系统队列已经满了的情况下，或者是所有特定方式的线都被保留了，还没有被填满），那么这个访问将在下一个周期再次尝试。\n值得注意的是，并不是所有的指令都能到达该单元的写回阶段。所有的存储指令和加载指令在所有请求的缓存块被命中的情况下都会在循环功能中退出流水线。这是因为它们不需要等待互连的响应，可以绕过写回逻辑，将指令所请求的缓存线和已经返回的缓存线记在账上。\n（8）Cache软件模型gpu-cache.h实现了ldst_unit使用的所有缓存。常量缓存和数据缓存都包含一个成员tag_array对象，实现了保留和替换逻辑。probe()函数检查一个块地址而不影响相关数据的LRU位置，而access()是为了模拟一个影响LRU位置的查找，是产生未命中和访问统计的函数。MSHR的模型是用mshr_table类来模拟一个具有有限数量的合并请求的完全关联表。请求通过next_access()函数从MSHR中释放。\nread_only_cache类用于常量缓存并作为data_cache类的基类。这个层次结构可能有些混乱，因为R/W数据缓存是从read_only_cache延伸出来的。唯一的原因是它们共享很多相同的功能，除了处理数据缓存中的写的访问函数之外。二级缓存也是通过data_cache类实现的。\ntex_cache类实现了上面的架构描述中的纹理缓存。它没有使用tag_array或mshr_table，因为它的操作与传统的缓存有很大的不同。\n（9）线程块/CTA/Warp Group的调度线程块对SIMT Core的调度发生在shader_core_ctx::issue_block2core(...)。一个Core上可同时调度的最大线程块（或CTA或Warp Group）的数量由函数shader_core_config::max_cta(...)计算。这个函数根据程序指定的每个线程块的数量、每个线程寄存器的使用情况、共享内存的使用情况以及配置的每个Core最大线程块数量的限制，确定可以并发分配给单个SIMT Core的最大线程块数量。具体来说，如果上述每个标准都是限制因素，那么可以分配给SIMT Core的线程块的数量被计算出来。其中的最小值就是可以分配给SIMT Core的最大线程块数。\n在shader_core_ctx::issue_block2core(...)中，线程块大小首先被填充为warp大小的精确倍数。然后确定一个空闲的硬件线程ID的范围。每个线程的功能状态通过调用ptx_sim_init_thread进行初始化。通过调用shader_core_ctx::init_warps来初始化SIMT堆栈和warp状态。\n当每个线程结束时，SIMT Core调用register_cta_thread_exit(...)来更新活动线程块的状态。当一个线程块中的所有线程都完成后，同一个函数会减少Core上活跃的线程块的数量，允许更多的线程块在下一个周期被安排。要调度的新线程块从待定Core中选择。\n3. 互联网络互连网络接口有以下几个功能。这些函数在interconnect_interface.cpp中实现。这些函数被包裹在icnt_wrapper.cpp中。设置icnt_wrapper.cpp的初衷是为了让其他网络模拟器能够与GPGPU-Sim挂钩：\n\ninit_interconnect()：初始化网络模拟器。它的输入是互连网络的配置文件和SIMT Core集群和内存节点的数量。\ninterconnect_push()：它指定了一个源节点、一个目的节点、一个要传输的数据包的指针和数据包的大小（以字节为单位）。\ninterconnect_pop()：获取一个节点号作为输入，并返回一个指向在该节点等待弹出的数据包的指针。如果没有数据包，则返回NULL。\ninterconnect_has_buffer()：获取一个节点号和要发送的数据包大小作为输入，如果源节点的输入缓冲区有足够的空间，则返回1（true）。\nadvance_interconnect()：应该在每个互连时钟周期被调用。顾名思义，它在一个周期内执行网络的所有内部步骤。\ninterconnect_busy()：如果网络内有数据包在飞行，则返回1（true）。\ninterconnect_stats()：打印网络统计数据。\n\n3. 用于intersim的时钟域交叉（1）从网络中弹出一个数据包我们在输出端每个虚拟Channel都有一个两级缓冲器，第一级包含每个虚拟Channel的缓冲器，其空间与网络内部的缓冲器相同，每一个虚拟Channel的下一级缓冲器是我们从一个时钟域跨越到另一个时钟域的地方。我们在互连时钟域将flit推入第二级缓冲器，并在Shader或L2/DRAM时钟域将整个数据包从第二级缓冲器中移除。只有当我们能够将flit从第一级缓冲区移动到第二级缓冲区时，我们才会返回一个Credit（这发生在互连时钟频率下）。\n（2）弹出接口细节下面是对时钟边界实现的更详细的解释。在每个路由器的弹出端口，我们有和虚拟Channel数量一样多的缓冲器。每个缓冲区的大小正好等于虚拟Channel缓冲区的大小。这些是上面提到的第一阶段的缓冲区。让我们把第二阶段的缓冲区（同样与虚拟Channel的数量相同）称为边界缓冲区。这些缓冲区的大小默认为每个16位（这是一个叫做boudry_buf_size的可配置选项）。当一个路由器试图弹出一个flit时，该flit会根据它来自的虚拟Channel被放入相应的第一阶段缓冲区（还没有送回credit）。然后检查边界缓冲区是否有空间；如果有空间，就从相应的弹出缓冲区弹出flit并推到边界缓冲区（这是在同一周期内对所有缓冲区做的）。在这一点上，flit也被推到了一个credit返回队列中。路由器可以在每个网络周期从这个credit回报队列中弹出1个flit，并产生其相应的credit。Shader（或L2/DRAM）端在每个Shader或（DRAM/L2周期）弹出边界缓冲区，并获得一个完整的 “数据包”，即如果数据包是4个flits，它在边界缓冲区释放了4个槽；如果是1个flit，它只释放了1个flit。由于边界缓冲区和虚拟Channel一样多，Shader（或DRAM）以轮流的方式弹出它们。(每个周期只能得到1个数据包）在这种设计中，第一阶段的缓冲区总是有空间容纳来自路由器的数据包，当边界缓冲区满了之后，向后流动的credit就会停止。\n请注意，上面描述的实现只是我们在模拟器中实现接口逻辑的方式，而不一定是网络接口在实际硬件中的实际实现方式。\n\n（2）向网络注入一个数据包网络的每个节点都有一个输入缓冲区。这个输入缓冲区的大小可以通过互连配置文件中的input_buffer_size选项来配置。为了将数据包注入互连，首先要通过调用interconnect_has_buffer()检查输入缓冲区的容量。如果有足够的空间，数据包将通过调用interconnect_push()被推送到互连网络。这些步骤是在Shader时钟域（在内存阶段）和内存节点的互连时钟域中完成的。\n每次调用advance_interconnect()函数（在互连时钟域）都会从每个节点的输入缓冲区中取出数据包，并实际开始在网络中传输（如果可能的话）。\n4. 内存分区内存分区是由定义在l2cache.h和l2cache.cc中的memory_partition_unit类来建模的。这些文件还定义了mem_fetch_allocator的扩展版本partition_mf_allocator，用于由内存分区和L2 Cache产生mem_fetch对象（内存请求）。\n从#内存分区微架构模型部分描述的子组件来看，data_cache类型的成员对象为二级缓存建模，dram_t类型为片外DRAM Channel。各种队列是使用fifo_pipeline类来建模的。最小延迟的ROP队列被建模为一个ROP_delay_t结构的队列。rop_delay_t结构存储每个内存请求退出ROP队列的最小时间（推送时间+恒定ROP延迟）。m_request_tracket对象跟踪所有尚未被内存分区完全服务的飞行中请求，以确定内存分区当前是否处于活动状态。\n原子操作单元没有一个相关的类。这个组件只是通过在功能上执行离开L2-&gt;icnt队列的内存请求的原子操作来建模的。下一节将介绍进一步的细节。\n（1）内存分区的连接和流量gpgpu_sim::cycle()方法对GPGPU-Sim中的所有架构组件进行了计时，包括内存分区的队列、DRAM Channel和二级缓存Bank。\n该代码段：\n::icnt_push( m_shader_config-&gt;mem2device(i), mf-&gt;get_tpc(), mf, response_size );m_memory_partition_unit[i]-&gt;pop();\n将内存请求从内存分区的L2-&gt;icnt队列中注入互连。对memory_partition_unit::pop()的调用在功能上执行了原子指令。请求跟踪器也会在这里丢弃该内存请求的条目，表明内存分区已经完成了对该请求的服务。\n对memory_partition_unit::dram_cycle()的调用将内存请求从L2-&gt;dram队列移动到DRAM Channel，DRAM Channel到dram-&gt;L2队列，并循环片外GDDR3 DRAM内存。\n对memory_partition_unit::push()的调用将数据包从互连网络中弹出并传递给内存分区。请求跟踪器被通知到该请求。纹理访问被直接推入icnt-&gt;L2队列，而非纹理访问被推入最小延迟ROP队列。请注意，推送到icnt-&gt;L2和ROP队列中的操作被icnt-&gt;L2队列的大小所节制，这个大小在memory_partition_unit::full()方法中定义。\n对memory_partition_unit::cache_cycle()的调用对二级缓存Bank进行计时，并将请求移入或移出二级缓存。下一节将描述memory_partition_unit::cache_cycle()的内部结构。\n（2）L2 Cache模型在memory_partition_unit::cache_cycle()里面，调用：\nmem_fetch *mf = m_L2cache-&gt;next_access();\n为在填充的MSHR条目中等待的内存请求生成回复，如#MSHR描述中所述。填充响应，即对L2在读取未命中时产生的内存请求的响应信息，通过从dram-&gt;L2队列中弹出并调用以下方式传递给L2缓存：\nm_L2cache-&gt;fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);\n\n由L2产生的由于读未命中的填充请求被从L2的未命中队列中弹出并推入L2-&gt;Dram队列，该方法是调用：\nm_L2cache-&gt;cycle();\n\n退出icnt-&gt;L2队列的内存请求的L2访问是通过调用：\nenum cache_request_status status = m_L2cache-&gt;access(mf-&gt;get_partition_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);\n\n在L2缓存命中时，立即产生一个响应并推入L2-&gt;icnt队列。在未命中时，这里不产生请求，因为缓存类内部的代码已经在其未命中队列中产生了一个内存请求。如果二级缓存被禁用，那么内存请求将直接从icnt-&gt;L2队列推送到L2-&gt;dram队列。\n同样在memory_partition_unit::cache_cycle()中，内存请求被从ROP队列中弹出并插入icnt-&gt;L2队列中。\n（3）DRAM调度和时序模型DRAM的时序模型在文件dram.h和dram.cc中实现。该时序模型还包括一个FIFO调度器的实现。更复杂的FRFCFS调度器位于dram_sched.h和dram_sched.cc中。\n函数dram_t::cycle()表示一个DRAM周期。在每个周期中，DRAM从请求队列中弹出一个请求，然后调用调度器函数，让调度器根据调度策略选择一个请求来进行服务。在请求被发送到调度器之前，它们在DRAM延迟队列中等待一个固定数量的SIMT Core周期。这个功能也在dram_t::cycle()里面实现。\ncase DRAM_FIFO: scheduler_fifo(); break;case DRAM_FRFCFS: scheduler_frfcfs(); break;\n\n然后，DRAM时序模型会根据配置文件中指定的不同的时序约束条件，检查是否有银行准备好发出一个新的请求。这些约束条件在DRAM模型中由类似这样的变量来表示：\nunsigned int CCDc; //Column to Column Delay\n\n这些变量在每个周期结束时被递减。只有当一个动作的所有约束变量都达到0时，才会采取该动作。每个采取的动作都会将一组约束变量重置为它们的原始配置值。例如，当一个列被激活时，变量CCDc被重置为其原始配置值，然后每个周期递减1。在这个变量达到0之前，我们不能调度一个新的列。宏指令DEC2ZERO会递减一个变量，直到它达到0，然后它保持在0，直到另一个动作将其重置。\n（二）CUDA-Sim和GPGPU-Sim之间的接口时序模拟器（GPGPU-Sim）通过ptx_thread_info类与功能模拟器（CUDA-Sim）接口。m_thread成员变量是SIMT Core类shader_core_ctx中ptx_thread_info的数组，维护该SIMT Core中所有活动线程的功能状态。时序模型通过warp_inst_t类与功能模型进行通信，warp_inst_t类表示一个指令的动态实例，正在由一个warp执行。\n时序模型在仿真的以下三个阶段与功能模型进行交流：\n1. 解码在shader_core_ctx::decode()的解码阶段，时序模拟器从功能模拟器中获得指令，给定一个PC。这是通过调用ptx_fetch_inst函数完成的。\n\n2. 指令执行\n功能执行：时序模型通过调用ptx_thread_info类的ptx_exec_inst方法将线程的功能状态提前一个指令。这是在core_t::execute_warp_inst_t内完成的。时序模拟器传递要执行的指令的动态实例，而功能模型则相应地推进线程的状态。\nSIMT堆栈更新：在功能上执行了一条warp的指令后，定时模型通过向功能模型请求更新SIMT堆栈中的下一个PC。这发生在simt_stack::update里面。\n原子回调：如果指令是一个原子操作，那么指令的功能执行就不会在core_t::execute_warp_inst_t中发生。相反，在功能执行阶段，功能模拟器通过调用warp_inst_t::add_callback在warp_inst_t对象中存储一个指向该原子指令的指针。时序模拟器在请求离开二级缓存时执行这个回调函数（见#内存分区连接和流量）。\n\n3. 启动线程块当新的线程块在shader_core_ctx::issue_block2core中启动时，时序模拟器通过调用功能模型方法ptx_sim_init_thread初始化每线程的功能状态。此外，时序模型还通过从功能模型中获取起始PC来初始化SIMT堆栈和warp状态。\n（三）地址解码地址解码负责将线性地址转换为原始地址，用于访问DRAM中适当的行、列和Bank。地址解码还负责确定向哪个内存控制器发送内存请求。地址解码的代码可以在addrdec.h和addrdec.cc中找到；位于gpgpu-sim_root/src/gpgpu-sim/中。当在内核代码中遇到一个加载或存储指令时，一个内存获取对象被创建（定义在mem_fetch.h/mem_fetch.cc中）。创建后，mem_fetch对象通过调用ddrdec_tlx(new_addr_type addr /*linear address*/, addrdec_t *tlx /*raw address struct*/)解码线性地址。\n线性地址的解释可以通过在gpgpusim.config文件中设置-gpgpu_mem_address_mask为(0,1,2,3,5,6,14,15,16,100,103,106,160)中的一个来设置为13个预定义配置之一。这些配置指定了用于从线性地址中提取芯片（内存控制器）、行、列、Bank和Burst的位掩码。可以通过在gpgpusim.config文件中设置-gpgpu_mem_addr_mapping来选择一个自定义的映射，比如说：\n-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RRBBBCCC.CCCSSSSS\n\n其中R(r)=row, B(b)=bank, C(c)=column, S(s)=Burst, D(d) [not shown]=chip。\n另外，dramid@&lt;#&gt;意味着地址解码器将从&lt;#&gt;位开始插入dram/chip ID（从LSB开始计算），即dramid@8将从8位开始。\n（四）输出到AerialVision性能展示台在gpgpu_sim::cycle()中，gpgpu_sim::visualizer_printstat()（在gpgpu-sim/visualizer.cc中）在每个采样间隔被调用，将监测的性能指标的快照附加到一个日志文件中。这个日志文件是AerialVision的延时视图的输入。日志文件在创建时通过zlib进行压缩，以减少磁盘的使用。采样间隔可以通过选项-gpgpu_runtime_stat来配置。\ngpgpu_sim::visualizer_printstat()调用一组函数来对各种模块的性能指标进行采样：\n\ncflog_visualizer_gzprint()：为PC-Histogram生成数据（详见ISPASS 2010论文）。\nshader_CTA_count_visualizer_gzprint()：每个SIMT Core中活跃的CTA的数量。\nshader_core_stats::visualizer_print()：SIMT Core的性能指标，包括warp周期分解。\nmemory_stats_t::visualizer_print()：内存访问的性能指标。\nmemory_partition_unit::visualizer_print()：每个内存分区的性能指标。调用 dram_t::visualizer_print()。\ntime_vector_print_interval2gzfile()：内存访问的延时分布。\n\nPC-Histogram是通过两个类实现的：thread_insn_span和thread_CFlocality。这两个类都可以在gpgpu-sim/stat-tool.h/.cc中找到。它通过一个C接口与SIMT Core对接：\n\ncreate_thread_CFlogger()：为每个SIMT Core创建一个thread_CFlocality对象。\ncflog_update_thread_pc()：更新一个线程的PC。新的PC将被添加到这个线程所触及的PC列表中。\ncflog_visualizer_gzprint()：将当前采样区间的PC柱状图输出到日志文件。\n\n（五）直方图GPGPU-Sim提供了几种直方图数据类型，简化了任何指标的数值分解的生成。这些直方图类在histogram.h/.cc中实现：\n\nbinned_histogram：基本直方图，每个独特的整数值都占据一个bin。\npow2_histogram：二次幂直方图，每个bin代表输入值的log2。当一个指标的值可以跨越很大的范围（相差几个数量级）时，这很有用。\nlinear_histogram：一个直方图，每个bin代表一个由stride指定的数值范围。\n\n所有的直方图类都提供相同的接口：\n\nconstructor([stride], name, nbins, [bins])：创建一个具有给定名称的直方图，具有#nbins个bin，并由给定指针（可选）定位的bin。stride选项只对linear_histogram有效。\nreset_bins()：将所有的bin重置为零。\nadd2bin(sample)：向直方图中添加一个样本。\nfprint(fout)：打印直方图到给定的文件句柄。这里是输出格式：&lt;name&gt; = &lt;number of samples in each bin&gt; max=&lt;maximum among the samples&gt; avg=&lt;average value of the samples&gt;\n\n（六）Dump Pipeline参见#可视化逐周期微体系结构行为了解如何使用。\n顶层的Dump Pipeline代码在pgpu-sim/gpu-sim.cc中的gpgpu_sim::pipeline(...)中实现。它调用gpgpu-sim/shader.cc中的shader_core_ctx::display_pipeline(...)来实现每个SIMT Core的流水线状态。对于内存分区状态，它调用gpgpu-sim/l2cache.cc中的memory_partition_unit::print(...)。\n\n五、CUDA-sim - 功能仿真引擎src/cuda-sim目录包含实现GPGPU-Sim使用的功能仿真引擎的文件。为了提高灵活性，功能仿真引擎在每个向量通道的单个标量操作层面上解释指令。\n（一）关键对象描述kernel_info (&lt;gpgpu-sim_root&gt;/src/abstract_hardware_model.h/cc): \n\nkernel_info_t对象包含GPU网格和线程块的尺寸，与内核入口点相关的function_info对象，以及在参数内存中为内核参数分配的内存。\n\nptx_cta_info (&lt;gpgpu-sim_root&gt;/src/ptx_sim.h/cc): \n\n包含一个合作线程阵列（CTA）中的线程集合的线程状态（ptx_thread_info）。\n\nptx_thread_info (&lt;gpgpu-sim_root&gt;/src/ptx_sim.h/cc):\n\n包含单个标量线程（OpenCL中的work item）的功能仿真状态。这包括以下内容：\n寄存器值存储\n本地内存存储（OpenCL中的私有内存）\n共享内存存储（OpenCL中的本地内存）。注意，同一线程块/Work Wrap的所有标量线程都会访问相同的共享内存存储。\n程序计数器（PC）\n调用堆栈\n线程ID（网格启动中的软件ID，以及表明它在时序模型中占据哪个硬件线程槽的硬件ID) \n\n\n\n目前的功能模拟引擎是为了支持NVIDIA的PTX而开发的。PTX本质上是一种低级别的编译器中间表示法，但不是NVIDIA硬件所使用的实际机器表示法（即SASS）。由于PTX没有定义二进制表示法，因此GPGPU-Sim不会存储指令的二进制视图（例如，当你在本科计算机架构课程中学习指令集设计时就会了解到）。相反，PTX的文本表示法被解析为一个对象列表，有点类似于低级别的编译器中间表示法。\n单独的PTX指令在PTX函数中找到，这些函数要么是内核入口点，要么是可以在GPU上调用的子程序。每个PTX函数都有一个function_info对象：\nfunction_info (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc): \n\n包含一个可以进行功能模拟的静态PTX指令（ptx_instruction‘s）列表。\n对于内核入口点，将每个内核参数存储在一个映射m_ptx_kernel_param_info中；但是，对于OpenCL应用程序来说，这可能并不总是这样的。在OpenCL中，相关的常量内存空间可以通过两种方式分配。它可以在声明它的ptx文件中显式初始化，或者使用主机上的clCreateBuffer来分配它。在后面这种情况下，.ptx文件将包含一个参数的全局声明，但它将有一个未知的数组大小。因此，该符号的地址将不会被设置，需要在执行PTX之前在function_info::add_param_data(...)函数中设置。在这种情况下，内核参数的地址被存储在function_info对象中的一个符号表中。\n\n下面的列表描述了在GPGPU-Sim 3.x中用于表示指令的类层次结构。该层次结构的设计是为了支持未来对PTX以外的指令集的扩展，并将功能仿真对象与时序模型隔离。inst_t (&lt;gpgpu-sim_root&gt;/src/abstract_hardware_model.h/cc): \n\n包含一个与微架构相关的静态指令的抽象视图。这包括操作码类型、源和目的寄存器标识符、指令地址、指令大小、重合点指令地址、指令延迟和启动间隔，对于内存操作，还包括访问的内存空间。\n\nwarp_inst_t (&lt;gpgpu-sim_root&gt;/src/abstract_hardware_model.h/cc，源于inst_t): \n\n包含与微架构相关的动态指令的视图。这包括每个通道的动态信息，如掩码状态和访问的内存地址。为了支持全局内存原子操作的精确功能执行，该类包括一个回调接口，当原子内存操作到达性能模拟中的DRAM接口时，对其进行功能模拟。\n\nptx_instruction (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc，源于warp_inst_t):\n\n 包含一个动态指令的全部状态，包括功能模拟所需的接口。\n\n为了支持功能模拟，GPGPU-Sim必须访问CUDA和OpenCL内存模型中定义的各种内存空间的数据。这需要一种命名位置的方法和一个存储这些位置值的地方。\n为了命名位置，GPGPU-Sim在解析输入的PTX时首先建立了一个symbol table表示。这是用下面的类来完成的：symbol_table (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc): \n\n包含PTX中一个内存位置的文本表示（e.g., “%r2”, “input_data”, etc…）到一个包含数据类型和位置信息的符号对象的映射。\n\nsymbol (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc): \n\n包含有关数据的名称和类型以及它在模拟GPU内存空间中的位置（地址或寄存器标识符）的信息。还可以跟踪该名称在PTX源中的声明位置。\n\ntype_info 和 type_info_key (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc): \n\n包含关于数据对象类型的信息（在指令解释期间使用）。\n\noperand_info (&lt;gpgpu-sim_root&gt;/src/cuda-sim/ptx_ir.h/cc): \n\n一个包含指令源操作数的封装类，可以是寄存器标识符、内存操作数（包括置换模式信息）或即时操作数。\n\n在函数仿真中使用的动态数据值的存储使用了不同的寄存器和内存空间类。寄存器的值包含在ptx_thread_info::m_regs中，这是一个从符号指针到C联合类型ptx_reg_t的映射。寄存器的访问使用方法ptx_thread_info::get_operand_value()，它使用operand_info作为输入。对于内存操作数，该方法返回内存操作数的有效地址。编程模型中的每个内存空间都包含在一个类型为memory_space的对象中。GPU中所有线程可见的内存空间都包含在gpgpu_t中，并通过ptx_thread_info中的接口进行访问（例如，ptx_thread_info::get_global_memory）。\nmemory_space (&lt;gpgpu-sim_root&gt;/src/cuda-sim/memory.h/cc): \n\n用于实现功能模拟状态的内存存储的抽象基类。\n\nmemory_space_impl (&lt;gpgpu-sim_root&gt;/src/cuda-sim/memory.h/cc): \n\n为了优化功能模拟的性能，内存是用哈希表实现的。哈希表的块大小是模板类memory_space_impl的模板参数。\n\n（二）PTX提取根据配置文件，PTX是从cubin文件或使用cuobjdump来提取的。本节描述了提取PTX和其他信息的流程。下面的图显示了提取的可能流程。\n\n\n1. 从cubincuda_runtime_api.cc中的__cudaRegisterFatBinary( void *fatCubin )是负责提取PTX的函数。该函数由程序为每个CUDA文件调用。FatbinCubin是一个结构，包含与该CUDA文件相对应的不同版本的PTX和cubin。GPGPU-Sim提取最新版本的PTX，该版本不比forced_max_capability（在仿真参数中定义）新。\n2. 使用cuobjdump在CUDA 4.0及以后的版本中，用于提取ptx和sass的fat cubin文件已经不可用。cuobjdump是Nvidia随工具包提供的一个工具，可以从可执行文件中提取PTX、SASS以及其他信息。如果选项-gpgpu_ptx_use_cuobjdump被设置为1，那么GPGPU-Sim将调用cuobjdump从二进制文件中提取PTX，SASS和其他信息。如果转换为PTXPlus被启用，模拟器将调用cuobjdump_to_ptxplus来将SASS转换为PTXPlus。然后加载生成的程序。\n（三）PTX/PTXPlus加载当PTX/PTXPlus程序准备好后，gpgpu_ptx_sim_load_ptx_from_string(...)被调用。这个函数基本上使用Lex/Yacc来解析PTX代码并为该PTX文件创建符号表。然后调用add_binary(...)。这个函数将创建的符号表添加到CUctx结构中，该结构保存了所有的函数和符号表信息。gpgpu_ptxinfo_load_from_string(...)函数被调用，以便从PTXinfo文件中提取一些信息。这个函数在PTX文件上运行ptxas（CUDA工具包中的PTX汇编工具）并使用Lex和Yacc解析输出文件。它从ptxinfo文件中提取一些信息，比如每个内核使用的寄存器的数量。同时调用gpgpu_ptx_sim_convert_ptx_to_ptxplus(...)来创建PTXPlus。\n__cudaRegisterFunction(...)函数由应用程序为每个设备函数调用。这个函数在设备和主机函数之间生成一个映射。在register_function(...)里面，GPGPU-sim搜索与设备函数所在的fatCubin相关的符号表。该函数生成内核入口点和CUDA应用函数地址（主机函数）之间的映射。\n\n（四）PTXPlus支持\n1. PTXPlus转换GPGPU-Sim 3.1.0及以后的版本通过使用NVIDIA的cuobjdump工具实现对本地硬件ISA执行（PTXPlus）的支持。目前，PTXPlus仅支持CUDA 4.0。当PTXPlus被启用时，模拟器使用cuobjdump将CUDA二进制文件中包含的嵌入式SASS（NVIDIA的硬件ISA）图像提取为文本格式。然后，这个SASS的文本表示法被转换为我们自己的PTX扩展，称为PTXPlus，使用一个单独的可执行文件，称为cuobjdump_to_ptxplus。在转换过程中，需要比SASS文本表示法中更多的信息。这些信息是从ELF和PTX代码中获得的，这些代码也是用cuobjdump提取的。 cuobjdump_to_ptxplus将所有这些信息捆绑在一个PTXPlus文件中。#图PtxVsPtxplusRuntimeFlow描述了使用PTXPlus时运行时执行流程的微小差异。请注意，在CUDA可执行文件的编译过程中不需要任何改变。转换过程完全由GPGPU-Sim在运行时处理。请注意，#图PtxVsPtxplusRuntimeFlow中的流程与我们ISPASS 2009论文中的图3(b)中的流程不同。\n从PTX到PTXPlus的转换是由位于ptx_loader.cc中的gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus()完成的。 gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus()是由usecuobjdump()调用的，它传入SASS、PTX和ELF信息。gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus()在这些输入上调用cuobjdump_to_ptxplus。cuobjdump_to_ptxplus使用这三个输入来创建原始程序的最终PTXPlus版本，并从gpgpu_ptx_sim_convert_ptx_and_sass_to_ptxplus()返回。\n2. cuobjdump_to_ptxplus的操作cuobjdump_to_ptxplus使用三个文件来生成PTXPlus。首先，在执行cuobjdump_to_ptxplus之前，GPGPU-Sim对NVIDIA的cuobjdump输出的信息进行了解析，只是将这些信息分成了多个文件。对于每个部分（一个部分对应一个CUDA二进制文件），会生成三个文件：.ptx、.sass和.elf。这些文件只是对cuobjdump的输出进行了分割，所以可以很容易地被cuobjdump_to_ptxplus处理。下面是对每个文件的描述：\n\n.ptx: 包含对应于CUDA二进制文件的PTX代码\n.sass: 包含通过构建PTX代码生成的SASS。\n.elf: 包含ELF对象的文本转储。\n\ncuobjdump_to_ptxplus将对应于单个二进制文件的三个文件作为输入，生成一个PTXPlus文件。根据需要多次调用cuobjdump_to_ptxplus来转换多个二进制文件。每个文件都被解析，并生成一个详细的中间表示。然后调用多个函数以PTXPlus文件的形式输出这个表示。下面是对从每个文件中提取的信息的描述：\n\n.ptx：ptx文件被用来提取关于可用内核的信息，它们的函数签名，以及关于纹理的信息。\n.sass：sass文件用于提取实际的指令，这些指令将在输出的PTXPlus文件中被转换为PTXPlus指令。\n.elf：elf文件用于提取常量和局部内存值以及常量内存指针。\n\n3. PTXPlus的实现ptx_thread_info::get_operand_value()在instructions.cc中决定了一个输入操作数的当前值。以下对get_operand_value的扩展是为了PTXPlus的执行：\n\n如果一个寄存器操作数有一个.lo修饰符，只读取低16位。如果一个寄存器的操作数有一个.hi修饰符，只有较高的16位被读取。这个信息被存储在操作数的m_operand_lohi属性中。值为0是默认的，值为1意味着.lo修饰符，值为2意味着.hi修饰符。\n对于PTXPlus风格的64位或128位操作数，get_reg()函数传递给每个寄存器名称，最终结果由每个寄存器中的数据组合而成。\nget_double_operand_type()的返回值表明使用了确定内存地址的新方法之一：\n如果是1，两个寄存器的值必须加在一起。\n如果是2，地址被存储在一个寄存器中，寄存器中的值被第二个寄存器中的值后递增。\n如果是3，地址被储存在一个寄存器中，寄存器中的值被一个常数后递增。\n\n\n对于内存操作数，get_operand_value()函数的前半部分计算出要访问的内存地址。这个值存储在result中。result被用作地址，适当的数据被从操作数指示的地址空间中获取并返回。对于增加后的内存访问，持有地址的寄存器也在get_operand_value()中被增加。\n如果它不是一个内存操作数，则返回该寄存器的值。\nget_operand_value会检查操作数是否为负号，如果有必要，在返回之前会取finalResult的负值。\n\n（五）控制流分析+预解码每个内核函数在加载到GPGPU-Sim时都会被分析和预解码。当PTX解析器检测到一个内核函数的结束时，它会调用function_info::ptx_assemble()（在cuda-sim/cuda-sim.cc中）。这个函数做了以下工作：\n\n给函数中的每条指令分配一个唯一的PC\n将函数中的每个分支标签解析为对应的指令/PC\n即确定每条分支指令的分支目标 \n\n\n为该函数创建控制流图\n进行控制流分析\n预先对每条指令进行解码（以加快仿真速度）\n\n控制流图的创建是通过function_info中的两个成员函数完成的：\n\ncreate_basic_blocks()：将各个指令分组为基本块（basic_block_t）。\nconnect_basic_blocks()：将基本块连接起来，形成控制流图。\n\n创建控制流图后，将进行两个控制流分析：\n\n确定每个中断指令的目标。\n这是一个临时性的解决方案，以支持PTXPlus中的中断指令，它在while-loops中实现了中断语句。一个长期的解决方案是用适当的中断条目扩展SIMT堆栈。\n目标是确定每个中断指令之前的最新的breakaddr指令。假设代码有结构化的控制流。这一信息可以通过上游遍历dominator树（通过调用成员函数find_dominators()和find_idominators()构建）而确定。然而，控制流图在中断指令连接到其目标后会发生变化。目前的解决方案是反复进行这种分析，直到dominator树和中断目标都变得稳定。\n寻找dominator的算法在Muchnick&#39;s Adv. &lt;&lt;Compiler Design and Implementation&gt;&gt;（图7.14和图7.15）中描述了寻找dominator的算法。\n\n\n找到每个分支指令的直接post-dominator。\n这一信息被SIMT堆栈用于分歧分支的重合点。\n分析是通过调用成员函数find_postdominators()和find_ipostdominators()完成的。该算法在Muchnick&#39;s Adv. &lt;&lt;Compiler Design and Implementation&gt;&gt;（图7.14和图7.15）中描述。\n\n\n\n预解码是通过调用ptx_instruction::pre_decode()对每条指令进行的。它提取了对定时模拟器有用的信息：\n\n检测LD/ST指令。\n确定该指令是否写到目标寄存器。\n如果这是一条分支指令，获得重合PC。\n提取该指令的寄存器操作数。\n检测预设指令。\n\n提取的信息存储在与指令对应的ptx_instruction对象中。这加快了仿真的速度，因为在一个内核启动中的所有标量线程都执行相同的内核函数。在函数加载时提取一次这些信息，比在仿真过程中对每个线程重复提取要有效得多。\n（六）内存空间缓冲器在CUDA-sim中，CUDA/OpenCL的各种内存空间是通过内存空间缓冲器（memory.h中的memory_space_impl类）来实现的：\n\n所有的全局、纹理、常量内存空间都是由顶级的gpgpu_t类中的一个memory_space对象实现的（作为成员对象m_global_memory）。\n每个线程的本地内存空间都包含在对应于每个线程的ptx_thread_info对象中。\n共享内存空间是整个CTA（线程块）所共有的，当每个CTA被分派执行时（在函数ptx_sim_init_thread()中），为其分配一个唯一的memory_space对象。当CTA执行完毕后，该对象被取消分配。\n\nmemory_space_impl类实现了由抽象类memory_space定义的读写接口。在内部，每个memory_space_impl对象包含一组内存页（由类模板mem_storage实现）。它使用STL无序Map（如果无序Map不可用，则恢复为STL Map）来将页与它们相应的地址联系起来。每个mem_storage对象是一个具有读写功能的字节数组。最初，每个memory_space对象是空的，当访问内存空间中单个页面对应的地址时（通过LD/ST指令或cudaMemcpy()），页面被按需分配。\nmemory_space、memory_space_impl和mem_storage的实现可以在memory.h和memory.cc文件中找到。\n（七）全局/常量内存初始化在CUDA中，程序员可以声明所有内核/设备函数都能访问的设备变量。这些变量可以在全局内存空间（如x_d）或常量内存空间（如y_c）：\n__device__ int x_d = 100; __constant__ int y_c = 70; \n\n这些变量和它们的初始值被编译成PTX变量。\n在GPGPU-Sim中，这些变量通过PTX解析器被解析为(symbol, value)对。在所有的PTX被加载后，两个函数，load_stat_globals(...)和load_constants(...)被调用，以便在模拟的全局内存空间中为每个变量分配一个内存地址，并将初始值复制到指定的内存位置。这两个函数位于cuda_runtime_api.cc中。\n这些变量在CUDA中也可以被声明为__global__。在这种情况下，它们可以被主机（CPU）和设备（GPU）访问。CUDA通过在主机内存和设备内存中都有同一变量的两个副本来实现这一目的。这两个副本之间的联系是通过函数__cudaRegisterVar(...)建立的。GPGPU-Sim拦截对该函数的调用以获取该信息，并通过调用函数gpgpu_ptx_sim_register_const_variable(...)或gpgpu_ptx_sim_register_global_variable(...)（在cuda-sim/cuda-sim.cc中实现）建立类似联系。有了这个链接，主机可以调用cudaMemcpyToSymbol()或cudaMemcpyFromSymbol()来访问这些__global__变量。GPGPU-Sim通过cuda-sim/cuda-sim.cc中的gpgpu_ptx_memcpy_symbol(...)实现这些函数。\n请注意，__cudaRegisterVar(...)不是CUDA Runtime API的一部分，未来的CUDA版本可能会以不同的方式实现__global__变量。在这种情况下，GPGPU-Sim将需要修改以支持新的实现。\n（八）内核启动：参数挂接GPGPU-Sim中的内核参数的设置方法与普通的CUDA和OpenCL应用程序相同：\nCUDA:kernel_name &lt;&lt;x,y&gt;&gt; (param1, param2, ..., paramN);OpenCL:clSetKernelArg(kernel_name, arg_index, arg_size, arg_value);\n\n另一种在CUDA中传递内核参数的方法是使用cudaSetupArgument(void* arg, size_t count, size_t offset)。这个函数从参数传递区的起始处（从offset=0开始）将arg所指向的参数的count个字节推到offset个字节处。参数被存储在执行栈的顶部。例如，如果一个CUDA内核有3个参数，a，b和c（按这个顺序），a的offset是0，b的offset是sizeof(a)，c的offset是sizeof(a)+sizeof(b)。\n对于CUDA和OpenCL，GPGPU-Sim为每个内核参数创建一个gpgpu_ptx_sim_arg对象，并维护一个所有内核参数的列表。在执行内核之前，会调用一个初始化函数来设置GPU网格的尺寸和参数：gpgpu_cuda_ptx_sim_init_grid(...)或者gpgpu_opencl_ptx_sim_init_grid(...)。在这些函数中使用了两个主要的对象，function_info和kernel_info_t对象，这些对象在上面有描述。在init_grid函数中，内核参数通过调用function_info_object::add_param_data(arg #, gpgpu_ptx_sim_arg *)被添加到function_info对象中。\n在向function_info对象添加所有的参数后，调用function_info::finalize(...)，它将存储在function_info::m_ptx_kernel_param_info映射中的内核参数复制到上面提到的kernel_info_t对象中分配的参数内存。如果之前在function_info::add_param_data(...)中没有做，每个内核参数的地址会被添加到function_info对象的符号表中。\n支持PTXPlus需要将内核参数复制到共享内存中。内核参数可以通过调用function_info::param_to_shared(shared_mem_ptr, symbol_table)函数从Param内存拷贝到Shared内存。这个函数遍历存储在function_info::m_ptx_kernel_param_info映射中的内核参数，并将每个参数从Param内存拷贝到ptx_thread_info::shared_mem_ptr指向的Shared内存中的适当位置。\nfunction_info::add_param_data(...), function_info::finalize(...), function_info::param_to_shared(...), 和gpgpu_opencl_ptx_sim_init_grid(...)函数在&lt;gpgpu-sim_root&gt;/distribution/src/cuda-sim/cuda-sim.cc中定义。gpgpu_cuda_ptx_sim_init_grid(...)函数在&lt;gpgpu-sim_root&gt;/distribution/libcuda/cuda_runtime_api.cc中实现。\n（九）通用内存空间通用寻址是NVIDIA PTX 2.0中引入的一项功能，指令ld、ldu、st、prefetch、prefetchu、isspacep、cvta、atom和red支持通用寻址。在通用寻址中，一个地址映射到全局内存，除非它位于本地内存窗口或共享内存窗口内。在这些窗口内，一个地址映射到本地或共享内存中的相应位置，即从通用地址中减去窗口基数形成的地址，以形成隐含状态空间的偏移。所以一条指令可以使用通用寻址来处理可能对应于全局、本地或共享内存空间的地址。\nGPGPU-Sim中的通用寻址是由指令ld, ldu, st, isspacep和cvta支持的。函数generic_to_&#123;shared, local, global&#125;, &#123;shared, local, global&#125;_to_generic和isspace_&#123;shared, local, global&#125;（都定义在cuda_sim.cc中）被用来支持GPGPU-Sim中的通用寻址，并使用前面提到的指令。\n标识符（SHARED_GENERIC_START, SHARED_MEM_SIZE_MAX, LOCAL_GENERIC_START, TOTAL_LOCAL_MEM_PER_SM, LOCAL_MEM_SIZE_MAX, GLOBAL_HEAP_START和STATIC_ALLOC_LIMIT）被定义在abstract_hardware_model.h中，用于定义不同内存空间边界（窗口）。这些标识符被用来推导和生成不同的地址空间，这也是支持通用寻址的需要。\n下表显示了代码中如何定义这些空间的例子：| 标识符 | 值 ||:—-|:—-||GLOBAL_HEAP_START|     0x80000000|SHARED_MEM_SIZE_MAX|     64*1024|LOCAL_MEM_SIZE_MAX|     8*1024|MAX_STREAMING_MULTIPROCESSORS|     64|MAX_THREAD_PER_SM|     2048|TOTAL_LOCAL_MEM_PER_SM|     MAX_THREAD_PER_SM*LOCAL_MEM_SIZE_MAX|TOTAL_SHARED_MEM|     MAX_STREAMING_MULTIPROCESSORS*SHARED_MEM_SIZE_MAX|TOTAL_LOCAL_MEM|MAX_STREAMING_MULTIPROCESSORS*MAX_THREAD_PER_SM*LOCAL_MEM_SIZE_MAX|SHARED_GENERIC_START|     GLOBAL_HEAP_START-TOTAL_SHARED_MEM|LOCAL_GENERIC_START|     SHARED_GENERIC_START-TOTAL_LOCAL_MEM|STATIC_ALLOC_LIMIT|     GLOBAL_HEAP_START - (TOTAL_LOCAL_MEM+TOTAL_SHARED_MEM) \n注意，在这种地址空间划分下，每个线程最多只能有8kB的本地内存（LOCAL_MEM_SIZE_MAX）。在CUDA计算能力1.3及以下版本中，每个线程最多可以有16kB的本地内存。在CUDA计算能力2.0的情况下，这一限制增加到512kB[7]。用户可以增加LOCAL_MEM_SIZE_MAX来支持每个线程需要超过8kB本地内存的应用。然而，应该始终确保GLOBAL_HEAP_START &gt; (TOTAL_LOCAL_MEM + TOTAL_SHARED_MEM)。如果不这样做，可能会导致错误的模拟行为。\n要获得更多关于通用寻址的信息，请参考NVIDIA的&lt;&lt;PTX: Parallel Thread Execution ISA Version 2.0 manual&gt;&gt;[8]。\n（十）指令执行在解析之后，用于功能执行的指令被表示为包含在function_info对象中的ptx_instruction对象（见cuda-sim/ptx_ir.&#123;h，cc&#125;）。每个标量线程由一个ptx_thread_info对象表示。执行一条指令（功能上）主要是通过调用ptx_thread_info::ptx_exec_inst()完成的。\n抽象类core_t拥有指令执行功能上所需的最基本的数据结构和程序。这个类是shader_core_ctx和functionalSimCore的基类，这两个类分别用于性能模拟和纯功能模拟。core_t最重要的成员是simt_stack和ptx_thread_info类型的对象，它们在功能模拟中用于跟踪warp分支分歧和处理线程的指令执行。\n执行指令简单地从使用函数ptx_sim_init_thread（在cuda-sim.cc中）初始化标量线程开始，然后我们使用function_info::ptx_exec_inst()在warp中执行标量线程。在这个版本中，保持线程作为warp的跟踪是通过为每个标量线程的warp使用一个simt_stack对象来完成的（这是这里假设的模型，也可以使用其他模型代替），simt_stack告诉我们哪些线程是活动的，在每个周期执行哪些指令，这样我们就可以在warp中执行标量线程。\n在ptx_thread_info::ptx_exec_inst中，是指令真正被执行的地方。我们检查指令的操作码并调用相应的函数，文件opcodes.def包含用于执行每个指令的函数。每个指令函数都需要两个ptx_instruction和ptx_thread_info的参数，这两个参数用于接收指令和执行中的线程的数据。\n信息从ptx_exec_inst的执行中传回给执行warp的函数，通过修改以引用方式传递给ptx_exec_inst的warp_inst_t参数，所以对于原子，我们表示执行的warp指令是原子的，并添加一个回调到warp_inst_t，设置原子标志，然后由warp执行函数检查该标志，以便进行回调，用于执行原子（检查cuda-sim.cc中的functionalCoreSim::executeWarp）。\n正如你可能已经预料到的那样，在性能仿真中比纯功能仿真中进行了更多的通信。可以检查functionalSimCore（在cuda-sim&#123;.h,.cc&#125;中）的纯功能执行，以获得更多关于功能执行的细节。\n（十一）AerialVision中源代码视图的接口AerialVision中的源代码视图是一个可以绘制不同类型的指标和ptx源代码的视图。例如，我们可以绘制ptx源代码中每一行产生的DRAM流量。GPGPU-Sim将AerialVision中构建源代码视图所需的统计数据导出到AerialVision读取的统计文件中。\n如果定义了-enable_ptx_file_line_stats 1和-visualizer_enabled 1选项，GPGPU-Sim将保存带有统计数据的文件。文件的名称可以通过选项-ptx_line_stats_filename filename来指定。\n对于执行的ptx文件中的每一行，都有一行被添加到行统计文件中，格式如下：\nkernel line : count latency dram_traffic smem_bk_conflicts smem_warp gmem_access_generated gmem_warp exposed_latency warp_divergence\n\n使用AerialVision，人们可以以不同的方式绘制/查看这个界面收集的统计数据。AerialVision还可以将这些统计数据映射到CUDA C++源文件中。关于如何做的更多细节，请参考AerialVision手册。\n这个功能在src/cuda-sim/ptx-stats.h/.cc中实现。每条ptx线的统计信息被保存在ptx_file_line_stats类的一个实例中。函数void ptx_file_line_stats_write_file()负责以上述格式将统计数据打印到统计文件中。其他一些函数，类似于void ptx_file_line_stats_add_dram_traffic(unsigned pc, unsigned dram_traffic)被模拟器的其他部分调用，以记录关于ptx源代码行的不同统计数据。\n（十二）纯功能仿真纯功能仿真（绕过性能仿真）在文件cuda-sim&#123;.h,.cc&#125;中实现，在函数gpgpu_cuda_ptx_sim_main_func(...)和使用functionalCoreSim类。functionalCoreSim类继承自core_t抽象类，它包含了许多纯函数仿真以及性能仿真所使用的函数仿真数据结构和程序。\n\n六、与外部世界的接口GPGPU-Sim被编译成stub库，在运行时动态链接到CUDA/OpenCL应用程序。这些库拦截了打算由CUDA运行环境执行的调用，而初始化模拟器并在其上运行内核，而不是在硬件上。\n（一）入口点和流管理器GPGPU-Sim由函数GPGUSim_Init()初始化，该函数在CUDA或OpenCL应用程序进行第一次CUDA/OpenCL API调用时被调用。我们对CUDA/OpenCL API函数的实现要么直接调用GPGPUSim_Init()，要么调用GPGPUSim_Context()，后者反过来调用GPGPUSim_Init()。一个调用GPGPUSim_Context()的API实例是cudaMalloc()。请注意，通过利用静态变量，GPGUSim_Init()并不是在每次调用cudaMalloc()时都被调用。\n第一次调用GPGPUSim_Init()将调用位于gpgpusim_entrypoint.cc中的函数gpgpu_ptx_sim_init_perf()。在gpgpu_ptx_sim_init_perf()中，所有的环境变量、命令行参数和配置文件都被处理。基于这些选项，一个gpgpu_sim对象被实例化并分配给全局变量g_the_gpu。同时，一个stream_manager对象被实例化并分配给全局变量g_stream_manager。\nGPGPUSim_Init()还调用了位于gpgpusim_entrypoint.cc中的start_sim_thread()函数，start_sim_thread()创建了一个新的pthread，实际上负责运行仿真。对于OpenCL应用，模拟器的pthread运行gpgpu_sim_thread_sequential()，它模拟内核的执行，一次一个。对于CUDA应用，模拟器pthread运行gpgpu_sim_thread_concurrent()，它模拟多个内核的并发执行。在模拟的GPU上可能并发执行的最大内核数量由选项-gpgpu_max_concurrent_kernel来配置。\ngpgpu_sim_thread_sequential()将等待一个开始信号来启动模拟（g_sim_signal_start）。启动信号是由用于启动OpenCL性能仿真的gpgpu_opencl_ptx_sim_main_perf()函数设置的。\ngpgpu_sim_thread_concurrent()函数初始化了一次性能仿真器结构，然后进入一个循环，等待来自流管理器（由stream_manager.h/cc中的stream_manager类实现）的作业。流管理器本身从CUDA API对cudaStreamCreate()和cudaMemcpy()等函数的调用和内核启动中获得工作。\n（二）CUDA运行库（libcudart）在构建CUDA应用程序时，NVIDIA的nvcc将每个内核的启动转化为对CUDA运行时API的一系列调用，从而为内核的执行准备好GPU硬件。libcudart.so是NVIDIA提供的实现该API的库。为了使GPGPU-Sim能够拦截这些调用并在模拟器上运行内核，GPGPU-Sim也实现了这个库。该库的实现位于libcuda/cuda_runtime_api.cc中。由此产生的共享对象位于gpgpu-Sim_root/lib/&lt;build_type&gt;/libcudart.so。在LD_LIBRARY_PATH中加入这个路径，你就可以指示系统在运行时动态链接GPGPU-Sim，而不是NVIDIA提供的库，从而使模拟器能够运行你的代码。设置LD_LIBRARY_PATH应该按照GPGPU-Sim附带的README文件中的指示，通过setup_environment脚本完成。我们对libcudart的实现并不与所有版本的cuda兼容，因为NVIDIA在不同版本之间使用不同的接口。\n（三）OpenCL库（libopencl）与上面描述的libcuda类似，libopencl是一个包含在GPGPU-Sim中的库，实现libOpencl.so中的OpenCL API。GPGPU-Sim目前支持OpenCL v1.1。OpenCL函数调用被GPGPU-Sim拦截，由模拟器而不是物理硬件来处理。产生的共享对象位于&lt;gpgpu-sim_root&gt;/lib/&lt;build_type&gt;/libOpenCL.so。通过在LD_LIBRARY_PATH中加入这一路径，你可以指示系统在运行时动态链接GPGPU-Sim，而不是NVIDIA提供的库，从而使模拟器能够运行你的代码。设置LD_LIBRARY_PATH应该按照v3.x目录下的README文件中的指示，通过setup_environment脚本完成。\n由于GPGPU-Sim执行PTX，OpenCL应用程序必须被编译并转换为PTX。这由nvopencl_wrapper.cc（在&lt;gpgpu-sim_root&gt;/distribution/libopencl/中找到）处理。OpenCL内核被传递给nvopencl_wrapper，使用标准的OpenCL函数clCreateProgramWithSource(...)和clBuildProgram(...)进行编译，转换为PTX并存储到一个临时PTX文件(_ptx_XXXX)，然后被读入GPGPU-Sim。编译OpenCL应用程序需要一个能够支持OpenCL的物理设备。因此，可能需要在包含这样一个设备的远程系统上执行编译过程。GPGPU-Sim通过使用&lt;OPENCL_REMOTE_GPU_HOST&gt;环境变量来支持。如果有必要，编译和转换为PTX的过程将在远程系统上执行，并将产生的PTX文件返回给本地系统，以便读入GPGPU-Sim。\n下表提供了目前在GPGPU-Sim中实现的OpenCL函数列表。有关这些函数行为的更多细节，请参见OpenCL规范文件。GPGPU-Sim的OpenCL API实现可以在&lt;gpgpu-sim_root&gt;/distribution/libopencl/opencl_runtime_api.cc中找到：\n\n\n\nOpenCL API\n\n\n\nclCreateContextFromType(cl_context_properties *properties, cl_ulong device_type, void (*pfn_notify)(const char *, const void *, size_t, void *), void * user_data, cl_int * errcode_ret)\n\n\nclCreateContext( const cl_context_properties * properties, cl_uint num_devices, const cl_device_id *devices, void (*pfn_notify)(const char *, const void *, size_t, void *), void * user_data, cl_int * errcode_ret)\n\n\nclGetContextInfo(cl_context context, cl_context_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret )\n\n\nclCreateCommandQueue(cl_context context, cl_device_id device, cl_command_queue_properties properties, cl_int * errcode_ret)\n\n\nclCreateBuffer(cl_context context, cl_mem_flags flags, size_t size , void * host_ptr, cl_int * errcode_ret )\n\n\nclCreateProgramWithSource(cl_context context, cl_uint count, const char ** strings, const size_t * lengths, cl_int * errcode_ret)\n\n\nclBuildProgram(cl_program program, cl_uint num_devices, const cl_device_id * device_list, const char * options, void (pfn_notify)(cl_program / program /, void * / user_data */), void * user_data )\n\n\nclCreateKernel(cl_program program, const char * kernel_name, cl_int * errcode_ret)\n\n\nclSetKernelArg(cl_kernel kernel, cl_uint arg_index, size_t arg_size, const void * arg_value )\n\n\nclEnqueueNDRangeKernel(cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim, const size_t * global_work_offset, const size_t * global_work_size, const size_t * local_work_size, cl_uint num_events_in_wait_list, const cl_event * event_wait_list, cl_event * event)\n\n\nclEnqueueReadBuffer(cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read, size_t offset, size_t cb, void * ptr, cl_uint num_events_in_wait_list, const cl_event * event_wait_list, cl_event * event )\n\n\nclEnqueueWriteBuffer(cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write, size_t offset, size_t cb, const void * ptr, cl_uint num_events_in_wait_list, const cl_event * event_wait_list, cl_event * event )\n\n\nclReleaseMemObject(cl_mem /* memobj */)\n\n\nclReleaseKernel(cl_kernel /* kernel */)\n\n\nclReleaseProgram(cl_program /* program */)\n\n\nclReleaseCommandQueue(cl_command_queue /* command_queue */)\n\n\nclReleaseContext(cl_context /* context */)\n\n\nclGetPlatformIDs(cl_uint num_entries, cl_platform_id *platforms, cl_uint *num_platforms )\n\n\nclGetPlatformInfo(cl_platform_id platform, cl_platform_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret )\n\n\nclGetDeviceIDs(cl_platform_id platform, cl_device_type device_type, cl_uint num_entries, cl_device_id * devices, cl_uint * num_devices )\n\n\nclGetDeviceInfo(cl_device_id device, cl_device_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret)\n\n\nclFinish(cl_command_queue /* command_queue */)\n\n\nclGetProgramInfo(cl_program program, cl_program_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret )\n\n\nclEnqueueCopyBuffer(cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer, size_t src_offset, size_t dst_offset, size_t cb, cl_uint num_events_in_wait_list, const cl_event * event_wait_list, cl_event * event )\n\n\nclGetKernelWorkGroupInfo(cl_kernel kernel, cl_device_id device, cl_kernel_work_group_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret )\n\n\nclWaitForEvents(cl_uint /* num_events /, const cl_event * / event_list */)\n\n\nclReleaseEvent(cl_event /* event */)\n\n\nclGetCommandQueueInfo(cl_command_queue command_queue, cl_command_queue_info param_name, size_t param_value_size, void * param_value, size_t * param_value_size_ret )\n\n\nclFlush(cl_command_queue /* command_queue */)\n\n\nclGetSupportedImageFormats(cl_context context, cl_mem_flags flags, cl_mem_object_type image_type, cl_uint num_entries, cl_image_format * image_formats, cl_uint * num_image_formats)\n\n\nclEnqueueMapBuffer(cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_map, cl_map_flags map_flags, size_t offset, size_t cb, cl_uint num_events_in_wait_list, const cl_event * event_wait_list, cl_event * event, cl_int * errcode_ret )\n\n\n","categories":["GPGPU-Sim","GPGPU-Sim 3.x"],"tags":["GPGPU-Sim"]},{"title":"Hello World","url":"/2022/12/21/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server1$ hexo server\n\nMore info: Server\nGenerate static files1$ hexo generate\n\nMore info: Generating\nDeploy to remote sites1$ hexo deploy\n\nMore info: Deployment\n","categories":[],"tags":[]}]